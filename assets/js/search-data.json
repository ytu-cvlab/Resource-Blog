{
  
    
        "post0": {
            "title": "Week 4, Day 1 (Introduction to Reinforcement Learning Framework)",
            "content": "Lecture Notebook . You can download the notebooks file for today lecture here. . . Introduction to OpenAI Gym . Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. . Installation . First, we need to install gym in our local machine. To do this, simply install gym using pip. . !pip install gym . Gym Environments . The OpenAI Gym involves a diverse suite of physical simulation environments ranging from easy to challenging tasks that we can play with and test our reinforcement learning algorithms with it. These include Classic control games, Atari, MuJoCo, Robotics and much more. You can find more about gym environments here. . In this course, we will focus on Toy text, Classic control and Atari environments. . Classic control and Toy text . These environments include small-scale tasks, mostly from the RL literature. . . Atari . These include classic atari games, which had a big impact on reinforcement learning research. . . Creating your first environment . We can simply call gym.make(&quot;env_name&quot;) to create an environment object. Here &quot;env_name&quot; denotes the name of the environment we are calling. All the available names of the environments can be found here. . # importing openai gym library import gym import numpy as np # create classic cart-pole env env = gym.make(&#39;MountainCar-v0&#39;) # reset/initialize the env env.reset() for _ in range(200): env.step(env.action_space.sample()) # take a random action env.render() # render the environment # close the rendering env.close() . . We can interact with the environment by two main methods: . env.reset() | env.step(action)obs = env.reset() method initialize and returns an initial observation (or state) of the environment. We will learn more about gym observations later. . | obs_next, reward, done, info = env.step(action) method interacts with the environment by taking an action as an input and returns four values:obs_next(env.observation_space), reward(float), done(bool) and info(dict). . Spaces . Every gym environment comes with an action_space and an observation_space. The formats of action and observation of an environment are defined by env.action_space and env.observation_space respectively, which are of type Space. . Types of gym spaces: . gym.spaces.Discrete(n): fixed range of non-negative discrete numbers from 0 to n-1. | gym.spaces.Box: represents an n-dimensional box, where the upper and lower bounds of each dimension are defined by Box.low and Box.high. | . Lets explore these two spaces. . # import spaces module from gym from gym import spaces space = spaces.Discrete(8) # Set with 8 elements {0, 1, 2, ..., 7} space . . Discrete(8) . space.sample() . . 4 . import warnings low_value = np.array([0,0,-1]) high_value = np.array([1,1,1]) box = spaces.Box(low_value, high_value) box . . Box(-1.0, 1.0, (3,), float32) . box.low . . array([ 0., 0., -1.], dtype=float32) . box.high . . array([1., 1., 1.], dtype=float32) . We can now check what are the spaces of previous cart-pole example used. You can find more about the spaces of cart-pole environment here. . env = gym.make(&#39;CartPole-v0&#39;) print(env.action_space) print(env.observation_space) . . Discrete(2) Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32) . env.observation_space.low . . array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38], dtype=float32) . env.observation_space.high . . array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38], dtype=float32) . . Monte Carlo Methods . In this notebook, we will learn Monte Carlo Methods using Blackjack environment from OpenAI Gym. You can find more about this environment here. . . BlackJack Env . Lets explore gym blackjack environment. We begin by importing the necessary modules and packages. . import gym import numpy as np from collections import defaultdict import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import LinearSegmentedColormap from utilities import plot_v, plot_policy . . We can create an instance of the blackjack env by calling gym.make(&#39;Blackjack-v0&#39;). . env = gym.make(&#39;Blackjack-v0&#39;) . . Now, lets find the details information of observation and action spaces. . env.observation_space . . Tuple(Discrete(32), Discrete(11), Discrete(2)) . env.action_space . . Discrete(2) . Observation space (State) is a tuple of three discrete spaces. You can find the details information of the observation space of blackjack env here. . the players current sum $ in {0, 1, ldots, 31 }$ | the dealer&#39;s one showing card $ in {1, 2, ldots, 10 }$ | the player holds a usable ace or not (0 = false or 1 = true) | . Action space consists of two discrete values. You can find the details information of the action space of blackjack env here. . 0 = STICK | 1 = HIT | . Before training our agent with Monte Carlo Methods, lets play blackjack with a random policy. . # number of episodes to play num_of_games = 2 for episode in range(num_of_games): # initialize the env state = env.reset() while True: # print current observation state print(state) # select action (hit or stick) randomly action = env.action_space.sample() # interacts with the env state, reward, done, info = env.step(action) # break loop if wins or lose if done: print(&#39;End game! Reward: &#39;, reward) print(&#39;You won :) n&#39;) if reward &gt; 0 else print(&#39;You lost :( n&#39;) break . . (12, 1, False) End game! Reward: -1.0 You lost :( (14, 5, False) (15, 5, False) End game! Reward: -1.0 You lost :( . . Monte Carlo Prediction . We begin Monte Carlo Methods by implementing Every-visit and First-visit MC Prediction algorithms for action-values. . We will closely follow the example (5.1) of Sutton and Barto text book. . We begin by considering a policy that sticks if the player&#39;s sum is 20 or 21, and otherwise hits. . The following function implements this policy. The function accepts an instance of OpenAI Gym&#39;s Blackjack environment as an input and returns an episode as an output which is a list of (state, actions and rewards) tuples. . def play_single_episode(env): episode = [] state = env.reset() while True: # custom policy action = 0 if state[0] &gt;= 20 else 1 next_state, reward, done, info = env.step(action) episode.append((state, action, reward)) # (S0, A0, R1), (S1, A1, R2), ... state = next_state if done: break return episode . . Lets play Blackjack with the implemented policy. . env = gym.make(&#39;Blackjack-v0&#39;) num_of_games = 3 for i in range(num_of_games): print(play_single_episode(env)) . . [((16, 10, False), 1, -1.0)] [((15, 3, False), 1, 0.0), ((17, 3, False), 1, 0.0), ((18, 3, False), 1, -1.0)] [((12, 4, False), 1, -1.0)] . Now, we are ready to implement our MC Prediction algorithm. We will start with Every-Visit MC Prediction, which is easier to understand and implement. The pseudocode below is used to implement our algorithm. . . We will call this function every_visit_mc_prediction. . The function accepts four input arguments: . env: an instance of OpenAI Gym&#39;s Blackjack environment | num_of_episodes: number of episodes to play | episode_generator: generate an episode of $(S_{i-1}, A_{i-1} , R_i)$ tuples using custom policy | gamma: discount rate (default = 1.0) | . The function returns an action-value: . Q: Q-Table of state, action pairs $Q(s,a)$ | . def every_visit_mc_prediction(env, num_of_episodes, episode_generator, gamma=1.0): # initialize empty dictionaries of arrays N = defaultdict(lambda: np.zeros(env.action_space.n)) Q = defaultdict(lambda: np.zeros(env.action_space.n)) Returns = defaultdict(lambda: np.zeros(env.action_space.n)) for i in range(1, num_of_episodes+1): # generate a single episode (S0, A0, R1,..., ST, AT, RT) episode = episode_generator(env) # for each tuple in episode for i, (s, a, r) in enumerate(episode): # calculate expected discounted return G # from current state onwards G = sum([x[2]*(gamma**i) for i,x in enumerate(episode[i:])]) Returns[s][a] += G N[s][a] += 1.0 Q[s][a] = Returns[s][a] / N[s][a] return Q . . Lets run our Every-Visit MC Prediction algorithm. Set the desired number of episodes. . env.seed(0) num_of_episodes = 10000 # run every-visit mc prediction algorithm for n episodes Q = every_visit_mc_prediction(env, num_of_episodes, play_single_episode) . . Now, lets plot our predicted state-value function using our test policy. . # obtain the corresponding state-value function for our test policy V = dict((k, (k[0]&gt;=20)*v[0] + (k[0]&lt;20)*v[1]) for k, v in Q.items()) # plot the state-value function heatmap plot_v(V) . . We can also test the First-Visit MC Prediction algorithm by using the pseudocode provided below. . . We will call this function first_visit_mc_prediction. . def first_visit_mc_prediction(env, num_of_episodes, episode_generator, gamma=1.0): # initialize empty dictionaries of arrays N = defaultdict(lambda: np.zeros(env.action_space.n)) Q = defaultdict(lambda: np.zeros(env.action_space.n)) Returns = defaultdict(lambda: np.zeros(env.action_space.n)) for i in range(1, num_of_episodes+1): # generate a single episode (S0, A0, R1,..., ST, AT, RT) episode = episode_generator(env) # create first-visit-check set first_visit_check = set() # for each tuple in episode for i, (s, a, r) in enumerate(episode): # if s is already in visit-check set if s in first_visit_check: # skip this state continue # add state to set first_visit_check.add(s) # calculate expected discounted return G # from current state onwards G = sum([x[2]*(gamma**i) for i,x in enumerate(episode[i:])]) Returns[s][a] += G N[s][a] += 1.0 Q[s][a] = Returns[s][a] / N[s][a] return Q env.seed(0) num_of_episodes = 10000 # run every-visit mc prediction algorithm for n episodes Q = first_visit_mc_prediction(env, num_of_episodes, play_single_episode) . . # obtain the corresponding state-value function for our test policy V = dict((k, (k[0]&gt;=20)*v[0] + (k[0]&lt;20)*v[1]) for k, v in Q.items()) # plot the state-value function heatmap plot_v(V) . . . Monte Carlo Control . We just finish the first step in Generalized Policy Iteration (GPI), which is a Policy Evaluation step. We do this by implementing two different MC prediction algorithms. Lets move to the next step where we will improve our policy. . In this notebook, we will implement Every-Visit Constant-$ alpha$ MC Control algorithm. . $ epsilon$-Greedy Policy . We will start by implementing the $ epsilon$-greedy policy. . $ pi{(a|s)} leftarrow begin{cases} 1- epsilon+ epsilon/{|A(s)|} &amp; text{if } a= text{argmax}_a Q(s,a) epsilon/{|A(s)|} &amp; text{otherwise} end{cases}$ . number_of_actions = 4 policy = np.ones(number_of_actions) * 1/number_of_actions print(&quot;Initial Policy: &quot;,policy) . . Initial Policy: [0.25 0.25 0.25 0.25] . # Choose epsilon value epsilon = 1 # for all actions policy = np.ones(number_of_actions) * epsilon / number_of_actions print(policy) . . [0.25 0.25 0.25 0.25] . # only for maximum action max_action = 3 policy[max_action] = 1 - epsilon + (epsilon / number_of_actions) print(policy) . . [0.25 0.25 0.25 0.25] . np.random.choice(np.arange(number_of_actions), p=policy) . . 3 . We will call this function epsilon_greedy_action. . The function accepts three input arguments: . epsilon (value between 0 and 1) | number_of_actions (action space size) | Q (Action-Value function) | . The function returns action index, which is chosen using the epsilon-greedy policy. . def epsilon_greedy_action(epsilon, number_of_actions, Q): policy = np.ones(number_of_actions) * epsilon / number_of_actions max_action_index = np.argmax(Q) policy[max_action_index] = 1 - epsilon + (epsilon / number_of_actions) action = np.random.choice(np.arange(number_of_actions), p=policy) return action . . Lets implement similar episode generator function from MC Prediction using our epsilon-greedy policy. . def play_single_episode(env, Q, epsilon): episode = [] state = env.reset() while True: # epsilon-greedy policy action = epsilon_greedy_action(epsilon, env.action_space.n, Q[state]) next_state, reward, done, info = env.step(action) episode.append((state, action, reward)) # (S0, A0, R1), (S1, A1, R2), ... state = next_state if done: break return episode . . Every-Visit Constant-$ alpha$ MC Control . Now, we are ready to implement the Every-Visit Constant-$ alpha$ MC Control algorithm. The pseudocode below is used to implement our algorithm. . . We will call this function every_visit_mc_control. . The function accepts eight input arguments: . env: an instance of OpenAI Gym&#39;s Blackjack environment | num_of_episodes: number of episodes to play | episode_generator: generate an episode of $(S_{i-1}, A_{i-1} , R_i)$ tuples using epsilon-greedy policy | alpha: step-size parameter in update equation | gamma: discount rate (default = 1.0) | epsilon: epsilon value for policy (default = 1.0) | epsilon_decay: decay rate of the epsilon (default = 0.99999) | epsilon_min: minimum value of the epsilon (default = 0.05) | . The function returns an action-value: . Q: Q-Table of state, action pairs $Q(s,a)$ | . def every_visit_mc_control(env, num_of_episodes, episode_generator, alpha, gamma=1.0, epsilon=1.0, epsilon_decay=0.99999, epsilon_min=0.05): # initialize empty dictionary Q Q = defaultdict(lambda: np.zeros(env.action_space.n)) for i in range(1, num_of_episodes+1): # calculate epsilon value epsilon = max(epsilon*epsilon_decay, epsilon_min) # generate a single episode (S0, A0, R1,..., ST, AT, RT) # using epsilon-greedy policy episode = episode_generator(env, Q, epsilon) # for each tuple in episode for i, (s, a, r) in enumerate(episode): # calculate expected discounted return G # from current state onwards G = sum([x[2]*(gamma**i) for i,x in enumerate(episode[i:])]) Q[s][a] = Q[s][a] + alpha*(G - Q[s][a]) return Q # Hyperparameters num_of_episodes = 10000 alpha = 0.01 gamma = 1.0 epsilon = 1.0 epsilon_decay = 0.9999 epsilon_min = 0.09 # run every-visit constant-alpha mc control algorithm Q = every_visit_mc_control(env, num_of_episodes, play_single_episode, alpha, gamma, epsilon, epsilon_decay, epsilon_min) . . # obtain the corresponding state-value function V = dict((k,np.max(v)) for k, v in Q.items()) # plot the state-value function heatmap plot_v(V) . . # obtain the corresponding best policy policy = dict((k,np.argmax(v)) for k, v in Q.items()) # plot the state-value function heatmap plot_policy(policy) . . We can refer to Figure 5.2 of the Sutton and Barto text book for true optimal policy which is shown below. Feel free to tweak around the hyperparameters and compare your results with the true optimal policy. . . . References . Sutton and Barto Textbook (Reinforcement Learning: An Introduction) | David Silver&#39;s Lecture (DeepMind) | Udacity&#39;s DeepRL Github Repo | . . Further Resources . Implementation of RL algorithms Github Repo | .",
            "url": "https://ytu-cvlab.github.io/mce-51069/deep_learning/reinforcement_learning/2020/12/28/week4-day1.html",
            "relUrl": "/deep_learning/reinforcement_learning/2020/12/28/week4-day1.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Week 3, Day 3 (Unsupervised Learning)",
            "content": "Materials and Assignments . Resources for today&#39;s materials can be downloaded from this link. Assignment notebooks are already included in zip file. You can also directly access to Assignment 5 Google Colab notebook from this link. . Dataset for Assignment (5) . Christmas trees dataset is used for this assignment. You can download the dataset separately from this link. ~50 images are left unlabelled in the dataset and you will need to label them manually. . Assignment Submission . Week 3 assignment deadline is 30th December 2020, 11:59 A.M. Submit your assignments from this link. . Unsupervised Learning : k-Means Clustering . One of the most widely used methods of clustering is k-means, which also tends to be a rather basic model. . Import libraries . Let&#39;s import some libraries necessary for our experiments . %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns; sns.set() # for plot styling import numpy as np from sklearn.model_selection import train_test_split import cv2 import pandas as pd import random . Straight Line . First, let&#39;s run k-means on a straight line to see what happens. Here, we have generated 30 data points on X axis. . X1 = 5*np.random.rand(30,1) X2 = np.zeros((30,1)) # X1.reshape(-1, 1) # X2.reshape(-1, 1) plt.scatter(X1,X2) X = np.hstack((X1,X2)) . Making sure the samples are on the first dimension . np.shape(X) . (30, 2) . Now, we will tell k-means to divide it into three clusters . from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3) kmeans.fit(X) y_kmeans = kmeans.predict(X) . plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=20, cmap=&#39;viridis&#39;) centers = kmeans.cluster_centers_ plt.scatter(centers[:, 0], centers[:, 1], c=&#39;black&#39;, s=150, alpha=0.5); . We will now try out k-means on sklearn generated data. Let&#39;s make five random blobs for this test. . from sklearn.datasets.samples_generator import make_blobs X, y_true = make_blobs(n_samples=500, centers=5, cluster_std=0.8, random_state=3) plt.scatter(X[:, 0], X[:, 1], s=20); . Thus, we assign k=5, . from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=5) kmeans.fit(X) y_kmeans = kmeans.predict(X) . Let&#39;s visualize the data as usual. . plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=20, cmap=&#39;viridis&#39;) centers = kmeans.cluster_centers_ plt.scatter(centers[:, 0], centers[:, 1], c=&#39;black&#39;, s=150, alpha=0.5); iterations = kmeans.n_iter_ print(iterations) . 2 . In this case, we discovered that all the means are converged to the center of each blob in 3 iterations . Custom dataset . Thus, let&#39;s try out k-means with our custom dataset. Iris dataset is yet again used for this experiment. . df = pd.read_csv(&#39;iris_data.csv&#39;) df.head() . sepal_length sepal_width petal_length petal_width variety . 0 5.1 | 3.5 | 1.4 | 0.2 | Setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | Setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | Setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | Setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | Setosa | . Extract X values for clustering . As k-means does not require labels, we assign all the features to X. . X1 = df.iloc[:,0:4] X1 = X1.values plt.scatter(X1[:, 2], X1[:, 3], s=50) # X1 = df.iloc[:,2:4] # X1 = X1.values # plt.scatter(X1[:, 0], X1[:, 1], s=50) . &lt;matplotlib.collections.PathCollection at 0x217502bdd90&gt; . As we have three classes in our dataset, we set k=3 . from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3, n_init= 10) kmeans.fit(X1) y1_kmeans = kmeans.predict(X1) . Let&#39;s plot our output on petal length and petal width axes- . plt.scatter(X1[:, 2], X1[:, 3], c=y1_kmeans, s=30, cmap=&#39;viridis&#39;, alpha = 0.6) centers = kmeans.cluster_centers_ plt.scatter(centers[:, 2], centers[:, 3], c=&#39;black&#39;, s=150, alpha=0.5); . Evaluation . As, K-means is a clustering model, it outputs clusters assigned to random integers and they may sometimes be of different order. Thus we convert our labels to integers accordingly to evaluate the model. . y1_kmeans . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1]) . y1 = df.loc[:,&#39;variety&#39;] y1 = y1.replace([&#39;Setosa&#39;,&#39;Versicolor&#39;,&#39;Virginica&#39;],[0,1,2]) y1 = y1.values y1 . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64) . from sklearn.metrics import accuracy_score accuracy_score(y1_kmeans, y1) . 0.8933333333333333 . Let&#39;s see what our confusion matrix looks like- . from sklearn.metrics import confusion_matrix import seaborn as sns mat = confusion_matrix(y1, y1_kmeans) sns.heatmap(mat.T, square=True, annot=True,fmt=&#39;d&#39;, cbar=False) plt.xlabel(&#39;true label&#39;) plt.ylabel(&#39;predicted label&#39;); . When not to use k-means . k-means has a leniar behavior and won&#39;t be optimal for all clusters. Let&#39;s see how k-means perform on a dataset with non-linear behavior. . from sklearn.datasets import make_moons X, y = make_moons(300, noise=.05, random_state=0) . labels = KMeans(2, random_state=0).fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap=&#39;viridis&#39;); . Thus, another method called SpectralClustering is used. We won&#39;t dig deeper into this method today. If you want to find out what it does, this article is recommended. . from sklearn.cluster import SpectralClustering model = SpectralClustering(n_clusters=2, affinity=&#39;nearest_neighbors&#39;, assign_labels=&#39;kmeans&#39;) labels = model.fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap=&#39;viridis&#39;); . Example : k-means for color compression . Color compression within images is another interesting application of k-means clustering. Imagine, for example, seeing an image with millions of colors. A large number of colors will not be used in most of the images, and many of the pixels in the image will have colors that are similar or even identical. In this experiment, we compressed. . plt.figure(figsize=(10,10)) img = cv2.imread(&quot;images/TDG.jpg&quot;) img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) ax = plt.axes(xticks=[], yticks=[]) ax.imshow(img); . img.shape . (810, 1080, 3) . Here, we stack the pixels to a single dimension so that it can fix into k-means . data = img / 255.0 # use 0...1 scale data = data.reshape(img.shape[0] * img.shape[1], 3) data.shape . (874800, 3) . Here, we have 874,800 data points to cluster and that could be a little too much for k-means. Thus, we use MiniBatchKMeans to fit our datapoints. . import warnings; warnings.simplefilter(&#39;ignore&#39;) # Fix NumPy issues. from sklearn.cluster import MiniBatchKMeans kmeans = MiniBatchKMeans(16) kmeans.fit(data) new_colors = kmeans.cluster_centers_[kmeans.predict(data)] . Let&#39;s recolor our image with these compressed colors and see the difference- . img_recolored = new_colors.reshape(img.shape) fig, ax = plt.subplots(1, 2, figsize=(16, 6), subplot_kw=dict(xticks=[], yticks=[])) fig.subplots_adjust(wspace=0.05) ax[0].imshow(img) ax[0].set_title(&#39;Original Image&#39;, size=16) ax[1].imshow(img_recolored) ax[1].set_title(&#39;16-color Image&#39;, size=16); . We can see that not much detail is lost during the process. and the colors are compressed efficiently . References . This lecture notebook is referenced from PythonDataScienceHandbook. Foloow thw link if you want more intuition about this model. .",
            "url": "https://ytu-cvlab.github.io/mce-51069/deep_learning/computer_vision/2020/12/26/week3-day3.html",
            "relUrl": "/deep_learning/computer_vision/2020/12/26/week3-day3.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Week 3, Day 2 (YOLO Model Training and Evaluation)",
            "content": "Notebook&#39;s from today&#39;s lecture is the based on the original yolov5 notebook modified by Roboflow. You can access to the notebook from this link . YOLO (You Only Look Once) . Before YOLO paper was released, region proposal networks combined with CNNs are usually used for object detection. These RPNs have high precision but takes a lot of time to train. Many attempts has been made to increase the speed of R-CNNs in the mid to late 2010s. . YOLO was introduced in 2015 and showed promise with its speed although it couldn&#39;t outperform other detection methods. Later, YOLOv3 was introduced and it made a breakthrough in object detection, outperforming all the object detectors. . Model Architecture . Convolutional Neural Network process the image the same way we human process the visual signal from our real world. It is a hierarchy process, in which we first detect shape, the structure and then object. . The model architecture of the Convolutional Neural Network is constructed just as the same as to first detect shape in the image, then the structure, and finally the object. By the difference process step in the model, it can be briefly divided into three part. . Backbone (To detect the basic visual data, such as shape, line) | Neck (Increase receptive field and connect higher layer feature with lower layer feature.) | Head (Perform specific task : object detection, semantic segmentation, etc.) | Backbone . In the research field of Deep learning in convolutional neural network, there are many state of the art backbone models that is proved to be useful in many difference field. Because of the basic visual information for all image data are similar, that is, shape, line and edge, so when researcher comes up with some idea of making a model, they often choose one of these state of the art as their backbone. . There are many backbone models available in computer vision field. Some of them are: . VGG model paper | ResNet model paper | MobileNet paper | CSPNet paper | Neck . To train a complex model, it is convention to add more layers to the model, while it can increase the model complexity, the model tend to forget early information in the later part of model. To avoid this, Neck layer connect between low level layers with high level layers, so that the early information in the model can last till the end of the model. Also, neck layer increase the receptive field of the model by various method. The most popular method is called the SPP(Spatial Pyramid Pooling) module, where the model use difference kernel size to convolute the output from the backbone. The neck layer for YOLOv4 is the comibination of PANet (Path-Aggregation Net) and SPP(Spatial Pyramid Pooling) module. . Head . The head of the model determine the task of the model. For example, for image classification model, the head would output the number of class in that dataset. While in the object detection, the head would output the bounding box location, the class of that bounding box and its confidence score, etc. The head of the model combine information feed from the neck and make the decision for the model. The head used in YOLO is called the YOLO head, where it performs only once for object detection. . How YOLO works . Grid cells . Yolo uses grid cells to identify objects. All grid cells are processed at once: hence, You Look Only Once. . . Anchor boxes . N anchor boxes are then introduced to each grid cell to detect the objects in a grid cell. The boxes can also extend outside of each grid cell if the centroid of the detected object falls into its region . . YOLO Layer . The output from the YOLO layer is determined by objectness, annotation dimensions and class confidence level for each class. Let&#39;s assume that we are detecting three classes. Thus, if we have 3x3 grid cells with 2 anchors, the output will be (3 x 3 x 16) . . . Non-max supression . Initially, each grid cell on the image generate n anchor boxes. . . YOLO then performs non-max supression on that anchor boxes to remove the anchor boxes with lower confidence levels. . . Recap . . Conclusion . As research efforts are poured into YOLO, developvers are putting more focus into optimizing the model. The fastest versions of YOLO v4 and v5 are regarded as state of the art object detection models available today and will remain in the top spot for the near future. . References . https://app.roboflow.com/ . https://www.coursera.org/lecture/convolutional-neural-networks/yolo-algorithm-fF3O0 . https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#6-visualize . https://www.analyticsvidhya.com/blog/2020/08/selecting-the-right-bounding-box-using-non-max-suppression-with-implementation/ . https://paperswithcode.com/method/yolov4 . https://medium.com/towards-artificial-intelligence/yolo-v5-explained-and-demystified-4e4719891d69 . https://www.coursera.org/lecture/convolutional-neural-networks/anchor-boxes-yNwO0 . https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006 . https://github.com/ultralytics/yolov5 . https://engineering.fb.com/2016/08/25/ml-applications/segmenting-and-refining-images-with-sharpmask/ .",
            "url": "https://ytu-cvlab.github.io/mce-51069/deep_learning/computer_vision/2020/12/23/week3-day2.html",
            "relUrl": "/deep_learning/computer_vision/2020/12/23/week3-day2.html",
            "date": " • Dec 23, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Week 3, Day 1 (Dataset Preparation and Arrangement)",
            "content": "You can download resources for today from this link. We have also posted a guide video on downloading and accessing materials on youtube channel. . . Datasets . Datasets comes in different forms from various sources. So the question here is what exactly is a dataset and how do we handle datasets for machine learning? To experiment the conditions, we must first know how to manipulate a dataset. . . Brief Introduction to Pandas . Pandas is a python library for data manipulation and analysis. In this section, we will feature a brief introuction to pandas. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import cv2 import math %matplotlib inline . Pandas stores data in dataframe objects. We can assign columns to each to numpy array (or) list to create a dataframe. . names = [&#39;Jack&#39;,&#39;Jean&#39;,&#39;Jennifer&#39;,&#39;Jimmy&#39;] ages = np.array([23,22,24,21]) # print(type(names)) # print(type(ages)) df = pd.DataFrame({&#39;name&#39;: names, &#39;age&#39;: ages, &#39;city&#39;: [&#39;London&#39;, &#39;Berlin&#39;, &#39;New York&#39;, &#39;Sydney&#39;]},index=None) df.head() # df.style.hide_index() . name age city . Jack | 23 | London | . Jean | 22 | Berlin | . Jennifer | 24 | New York | . Jimmy | 21 | Sydney | . Now, let&#39;s see some handy dataframe tricks. . df[[&#39;name&#39;,&#39;city&#39;]] . name city . 0 Jack | London | . 1 Jean | Berlin | . 2 Jennifer | New York | . 3 Jimmy | Sydney | . df.info() # print(df.columns) # print(df.age) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 4 entries, 0 to 3 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 name 4 non-null object 1 age 4 non-null int32 2 city 4 non-null object dtypes: int32(1), object(2) memory usage: 208.0+ bytes . Now that we know how to create a dataframe, we can save the dataframe we created. . df.to_csv(&#39;Ages_and_cities.csv&#39;,index=False,header=True) . df = pd.read_csv(&#39;Ages_and_cities.csv&#39;) df.head() . name age city . 0 Jack | 23 | London | . 1 Jean | 22 | Berlin | . 2 Jennifer | 24 | New York | . 3 Jimmy | 21 | Sydney | . . Understanding your dataset . In this section, we used Iris flowers dataset, which contains petal and sepal measurements of three species of Iris flowers. . Three species of Iris flowers from the dataset . . Sepal vs Petal . This dataset was introduced by biologist Ronald Fisher in his 1936 paper. The following figure explains the way length and width are mesured or petal and speal of each flower. . . Image source . When we observe the dataset, we will discover that the dataset has four features and three unique labels for three flowers. . df = pd.read_csv(&#39;iris_data.csv&#39;) df.head() # df.head(3) . sepal_length sepal_width petal_length petal_width variety . 0 5.1 | 3.5 | 1.4 | 0.2 | Setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | Setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | Setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | Setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | Setosa | . df.tail() . sepal_length sepal_width petal_length petal_width variety . 145 6.7 | 3.0 | 5.2 | 2.3 | Virginica | . 146 6.3 | 2.5 | 5.0 | 1.9 | Virginica | . 147 6.5 | 3.0 | 5.2 | 2.0 | Virginica | . 148 6.2 | 3.4 | 5.4 | 2.3 | Virginica | . 149 5.9 | 3.0 | 5.1 | 1.8 | Virginica | . . Slicing data . Now that we understand our dataset, let&#39;s prepare to seperate our data based on labels for unique visualization. . df.loc[80:85,(&quot;sepal_length&quot;,&quot;variety&quot;)] . sepal_length variety . 80 5.5 | Versicolor | . 81 5.5 | Versicolor | . 82 5.8 | Versicolor | . 83 6.0 | Versicolor | . 84 5.4 | Versicolor | . 85 6.0 | Versicolor | . # df.iloc[80:85,2:5] df.iloc[80:85,[0,4]] . sepal_length variety . 80 5.5 | Versicolor | . 81 5.5 | Versicolor | . 82 5.8 | Versicolor | . 83 6.0 | Versicolor | . 84 5.4 | Versicolor | . Se= df.loc[df.variety ==&#39;Setosa&#39;, :] Vc= df.loc[df.variety ==&#39;Versicolor&#39;, :] Vi= df.loc[df.variety ==&#39;Virginica&#39;, :] Vi.head() . sepal_length sepal_width petal_length petal_width variety . 100 6.3 | 3.3 | 6.0 | 2.5 | Virginica | . 101 5.8 | 2.7 | 5.1 | 1.9 | Virginica | . 102 7.1 | 3.0 | 5.9 | 2.1 | Virginica | . 103 6.3 | 2.9 | 5.6 | 1.8 | Virginica | . 104 6.5 | 3.0 | 5.8 | 2.2 | Virginica | . . Feature visualization . df = pd.read_csv(&#39;iris_data.csv&#39;) # df.dtypes . First, we will visualize each measurement with histograms to observe the output distribution for each class. . plt.figure(figsize=(15,15)) plt.subplot(2, 2, 1) plt.hist(Se.sepal_length,bins=15,color=&quot;steelblue&quot;,edgecolor=&#39;black&#39;,alpha =0.4, label=&quot;Setosa&quot;) plt.hist(Vc.sepal_length,bins=15,color=&#39;red&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Versicolor&quot;) plt.hist(Vi.sepal_length,bins=15,color=&#39;blue&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Virginica&quot;) plt.title(&quot;sepal length distribution&quot;), plt.xlabel(&#39;cm&#39;) plt.legend() plt.subplot(2, 2, 2) plt.hist(Se.sepal_width,bins=15,color=&quot;steelblue&quot;,edgecolor=&#39;black&#39;,alpha =0.4, label=&quot;Setosa&quot;) plt.hist(Vc.sepal_width,bins=15,color=&#39;red&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Versicolor&quot;) plt.hist(Vi.sepal_width,bins=15,color=&#39;blue&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Virginica&quot;) plt.title(&quot;sepal width distribution&quot;), plt.xlabel(&#39;cm&#39;) plt.legend() plt.subplot(2, 2, 3) plt.hist(Se.petal_length,bins=10,color=&quot;steelblue&quot;,edgecolor=&#39;black&#39;,alpha =0.4, label=&quot;Setosa&quot;) plt.hist(Vc.petal_length,bins=10,color=&#39;red&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Versicolor&quot;) plt.hist(Vi.petal_length,bins=10,color=&#39;blue&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Virginica&quot;) plt.title(&quot;petal length distribution&quot;), plt.xlabel(&#39;cm&#39;) plt.legend() plt.subplot(2, 2, 4) plt.hist(Se.petal_width,bins=10,color=&quot;steelblue&quot;,edgecolor=&#39;black&#39;,alpha =0.4, label=&quot;Setosa&quot;) plt.hist(Vc.petal_width,bins=10,color=&#39;red&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Versicolor&quot;) plt.hist(Vi.petal_width,bins=10,color=&#39;blue&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Virginica&quot;) plt.title(&quot;petal width distribution&quot;), plt.xlabel(&#39;cm&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1bb6f0b0370&gt; . Now, we will visualize multiple features with scatter plots to gain some more insights. . plt.figure(figsize=(15,15)) area = np.pi*20 plt.subplot(2, 2, 1) plt.scatter(Se.sepal_length,Se.sepal_width, s=area, c=&quot;steelblue&quot;, alpha=0.6, label=&quot;Setosa&quot;) plt.scatter(Vc.sepal_length,Vc.sepal_width, s=area, c=&quot;red&quot;, alpha=0.6, label=&quot;Versicolor&quot;) plt.scatter(Vi.sepal_length,Vi.sepal_width, s=area, c=&quot;blue&quot;, alpha=0.5, label=&quot;Virginica&quot;) plt.title(&quot;sepal length Vs sepal width&quot;), plt.xlabel(&#39;cm&#39;), plt.ylabel(&#39;cm&#39;) plt.legend() plt.subplot(2, 2, 2) plt.scatter(Se.petal_length,Se.petal_width, s=area, c=&quot;steelblue&quot;, alpha=0.6, label=&quot;Setosa&quot;) plt.scatter(Vc.petal_length,Vc.petal_width, s=area, c=&quot;red&quot;, alpha=0.6, label=&quot;Versicolor&quot;) plt.scatter(Vi.petal_length,Vi.petal_width, s=area, c=&quot;blue&quot;, alpha=0.5, label=&quot;Virginica&quot;) plt.title(&quot;petal length Vs petal width&quot;), plt.xlabel(&#39;cm&#39;), plt.ylabel(&#39;cm&#39;) plt.legend() plt.subplot(2, 2, 3) plt.scatter(Se.sepal_length,Se.petal_length, s=area, c=&quot;steelblue&quot;, alpha=0.6, label=&quot;Setosa&quot;) plt.scatter(Vc.sepal_length,Vc.petal_length, s=area, c=&quot;red&quot;, alpha=0.6, label=&quot;Versicolor&quot;) plt.scatter(Vi.sepal_length,Vi.petal_length, s=area, c=&quot;blue&quot;, alpha=0.5, label=&quot;Virginica&quot;) plt.title(&quot;sepal length Vs petal length&quot;), plt.xlabel(&#39;cm&#39;), plt.ylabel(&#39;cm&#39;) plt.legend() plt.subplot(2, 2, 4) plt.scatter(Se.sepal_width,Se.petal_width, s=area, c=&quot;steelblue&quot;, alpha=0.6, label=&quot;Setosa&quot;) plt.scatter(Vc.sepal_width,Vc.petal_width, s=area, c=&quot;red&quot;, alpha=0.6, label=&quot;Versicolor&quot;) plt.scatter(Vi.sepal_width,Vi.petal_width, s=area, c=&quot;blue&quot;, alpha=0.5, label=&quot;Virginica&quot;) plt.title(&quot;sepal width Vs petal width&quot;), plt.xlabel(&#39;cm&#39;), plt.ylabel(&#39;cm&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1bb6f4e9730&gt; . We can definitely see some blobs forming from these visualizations. &quot;Setosa&quot; class unsally stands out from the other two classes but the sepal width vs sepal length plot shows &quot;versicolor&quot; and &quot;virginica&quot; classes will more challenging to classify compared to &quot;setosa&quot; class. . . Training the model . Scikit-learn is a free machine learning library for Python which features various classification, regression and clustering algorithms. . Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics . from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn import metrics import seaborn as sns . df = pd.read_csv(&#39;iris_data.csv&#39;) # df.dtypes df.tail() . sepal_length sepal_width petal_length petal_width variety . 145 6.7 | 3.0 | 5.2 | 2.3 | Virginica | . 146 6.3 | 2.5 | 5.0 | 1.9 | Virginica | . 147 6.5 | 3.0 | 5.2 | 2.0 | Virginica | . 148 6.2 | 3.4 | 5.4 | 2.3 | Virginica | . 149 5.9 | 3.0 | 5.1 | 1.8 | Virginica | . train_X, test_X, train_y, test_y = train_test_split(df[df.columns[0:4]].values, df.variety.values, test_size=0.25) modelDT = DecisionTreeClassifier().fit(train_X, train_y) DT_predicted = modelDT.predict(test_X) modelRF = RandomForestClassifier().fit(train_X, train_y) RF_predicted = modelRF.predict(test_X) . . Model Evaluation . Decision Tree classifier . print(metrics.classification_report(DT_predicted, test_y)) . precision recall f1-score support Setosa 1.00 1.00 1.00 14 Versicolor 0.78 0.88 0.82 8 Virginica 0.93 0.88 0.90 16 accuracy 0.92 38 macro avg 0.90 0.92 0.91 38 weighted avg 0.93 0.92 0.92 38 . mat = metrics.confusion_matrix(test_y, DT_predicted) sns.heatmap(mat.T, square=True, annot=True, fmt=&#39;d&#39;, cbar=False) plt.xlabel(&#39;true label&#39;) plt.ylabel(&#39;predicted label&#39;); . Ramdom Forest Classifier . print(metrics.classification_report(RF_predicted, test_y)) . precision recall f1-score support Setosa 1.00 1.00 1.00 14 Versicolor 0.78 0.88 0.82 8 Virginica 0.93 0.88 0.90 16 accuracy 0.92 38 macro avg 0.90 0.92 0.91 38 weighted avg 0.93 0.92 0.92 38 . from sklearn.metrics import confusion_matrix import seaborn as sns mat = confusion_matrix(test_y, RF_predicted) sns.heatmap(mat.T, square=True, annot=True,fmt=&#39;d&#39;, cbar=False) plt.xlabel(&#39;true label&#39;) plt.ylabel(&#39;predicted label&#39;); . colab notebook . . Feature Engineering . When generating new features, the product between two features is usually not recommended to engineer unless it makes a magnification of the situation. Here, we use two new features, petal hypotenuse and petal product. . df = pd.read_csv(&#39;iris_data.csv&#39;) df[&#39;petal_hypotenuse&#39;] = np.sqrt(df[&quot;petal_length&quot;]**2+df[&quot;petal_width&quot;]**2) df[&#39;petal_product&#39;]=df[&quot;petal_length&quot;]*df[&quot;petal_width&quot;] df.tail() . sepal_length sepal_width petal_length petal_width variety petal_hypotenuse petal_product . 145 6.7 | 3.0 | 5.2 | 2.3 | Virginica | 5.685948 | 11.96 | . 146 6.3 | 2.5 | 5.0 | 1.9 | Virginica | 5.348832 | 9.50 | . 147 6.5 | 3.0 | 5.2 | 2.0 | Virginica | 5.571355 | 10.40 | . 148 6.2 | 3.4 | 5.4 | 2.3 | Virginica | 5.869412 | 12.42 | . 149 5.9 | 3.0 | 5.1 | 1.8 | Virginica | 5.408327 | 9.18 | . Se= df.loc[df.variety ==&#39;Setosa&#39;, :] Vc= df.loc[df.variety ==&#39;Versicolor&#39;, :] Vi= df.loc[df.variety ==&#39;Virginica&#39;, :] plt.figure(figsize=(16,8)) plt.subplot(1, 2, 1) plt.hist(Se.petal_hypotenuse,bins=10,color=&quot;steelblue&quot;,edgecolor=&#39;black&#39;,alpha =0.4 , label=&quot;Setosa&quot;) plt.hist(Vc.petal_hypotenuse,bins=10,color=&#39;red&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Versicolor&quot;) plt.hist(Vi.petal_hypotenuse,bins=10,color=&#39;blue&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Virginica&quot;) plt.legend() plt.title(&quot;petal hypotenuse distribution&quot;), plt.xlabel(&#39;cm&#39;) plt.subplot(1, 2, 2) plt.hist(Se.petal_product,bins=10,color=&quot;steelblue&quot;,edgecolor=&#39;black&#39;,alpha =0.4, label=&quot;Setosa&quot;) plt.hist(Vc.petal_product,bins=10,color=&#39;red&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Versicolor&quot;) plt.hist(Vi.petal_product,bins=10,color=&#39;blue&#39;,edgecolor=&#39;black&#39;, alpha =0.3, label=&quot;Virginica&quot;) plt.legend() plt.title(&quot;petal product distribution&quot;), plt.xlabel(&#39;cm&#39;) . (Text(0.5, 1.0, &#39;petal product distribution&#39;), Text(0.5, 0, &#39;cm&#39;)) . plt.figure(figsize=(10,10)) area = np.pi*20 plt.scatter(Se.petal_hypotenuse,Se.petal_product, s=area, c=&quot;steelblue&quot;, alpha=0.6, label=&quot;Setosa&quot;) plt.scatter(Vc.petal_hypotenuse,Vc.petal_product, s=area, c=&quot;red&quot;, alpha=0.6, label=&quot;Versicolor&quot;) plt.scatter(Vi.petal_hypotenuse,Vi.petal_product, s=area, c=&quot;blue&quot;, alpha=0.5, label=&quot;Virginica&quot;) plt.title(&quot;petal hypotenuse Vs petal product&quot;), plt.xlabel(&#39;cm&#39;), plt.ylabel(&#39;cm^2&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1bb72567f10&gt; . Train with Engineered features . Now, let&#39;s replace two petal features with two new features we generated. . df.head() . sepal_length sepal_width petal_length petal_width variety petal_hypotenuse petal_product . 0 5.1 | 3.5 | 1.4 | 0.2 | Setosa | 1.414214 | 0.28 | . 1 4.9 | 3.0 | 1.4 | 0.2 | Setosa | 1.414214 | 0.28 | . 2 4.7 | 3.2 | 1.3 | 0.2 | Setosa | 1.315295 | 0.26 | . 3 4.6 | 3.1 | 1.5 | 0.2 | Setosa | 1.513275 | 0.30 | . 4 5.0 | 3.6 | 1.4 | 0.2 | Setosa | 1.414214 | 0.28 | . df2 = df.loc[:,[&quot;sepal_length&quot;,&quot;sepal_width&quot;,&quot;petal_hypotenuse&quot;,&quot;petal_product&quot;,&quot;variety&quot;]] df2.dtypes . sepal_length float64 sepal_width float64 petal_hypotenuse float64 petal_product float64 variety object dtype: object . train_X, test_X, train_y, test_y = train_test_split(df2[df2.columns[0:4]].values, df2.variety.values, test_size=0.25) from sklearn.tree import DecisionTreeClassifier modelDT = DecisionTreeClassifier().fit(train_X, train_y) DT_predicted = modelDT.predict(test_X) from sklearn.ensemble import RandomForestClassifier modelRF = RandomForestClassifier().fit(train_X, train_y) RF_predicted = modelRF.predict(test_X) . print(metrics.classification_report(DT_predicted, test_y)) # print(metrics.classification_report(RF_predicted, test_y)) . precision recall f1-score support Setosa 1.00 1.00 1.00 14 Versicolor 0.90 1.00 0.95 9 Virginica 1.00 0.93 0.97 15 accuracy 0.97 38 macro avg 0.97 0.98 0.97 38 weighted avg 0.98 0.97 0.97 38 . from sklearn.metrics import confusion_matrix import seaborn as sns mat = confusion_matrix(test_y, DT_predicted) # mat = confusion_matrix(test_y, RF_predicted) sns.heatmap(mat.T, square=True, annot=True, fmt=&#39;d&#39;, cbar=False) plt.xlabel(&#39;true label&#39;) plt.ylabel(&#39;predicted label&#39;); . . Reference - Python Data Science Handbook . . Annotations . Image labels . For classification models, we have a single label for each set of images in the same class. Annotations can be made very easily. . . Bounding boxes . We usually use rectangular bounding boxes for object detection. Detection Models like YOLO and Faster-RCNN use this type of annotations. Bounding boxes are ususally represented by either the coordinates (x1,y1) lower left corner or (x2,y2) upper right corner of the box, followed by height and wigth of the bounding box. . . Segmentation . Polygonal Segmentation . Bounding boxes are simple but not ideal for all types of objects as we have to frame every object in a rectangular box. To solve this problem, polygonal segmentation is introduced. With this method, we can annotate the exact features of the objects with polygons. The image below is from one of my projects for segmentation of temples in ASEAN. . . Semantic Segmentation . This technique takes segmentation to the pixel level. A particular class is assigned to every pixel in the image. Semantic segmentation is used mainly in situations where there is a very significant environmental context. It is used, for instance, in self-driving cars and robotics so that the models understand the environment in which they operate. . . . Image Datasets . COCO dataset . Google&#39;s Open images V6 . ImageNet . CIFAR-10 . MNIST . PASCAL VOC . . Annotation tools . Makesense . LabelImg . References . Tensorflow, Google images , Sabina Pokhrel&#39;s article , Cityscapes dataset .",
            "url": "https://ytu-cvlab.github.io/mce-51069/deep_learning/computer_vision/2020/12/23/week3-day1.html",
            "relUrl": "/deep_learning/computer_vision/2020/12/23/week3-day1.html",
            "date": " • Dec 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Week 2, Day 3 (Guided Project with Pytorch on Image Classification)",
            "content": "Optional Lecture . For the better understanding of the model, we provide another notebook where you can inspect the model convolutional filter and its output. The link to the colab for that notebook is here . Assignment . For the assignment of Week 2, Please visit this link and copy the notebook to your google drive. . After training the network for MNIST dataset is finised, please submit the weight file save at the last cell to this link. . Import Necessary Libraries . Pytorch is the open source machine learning framework that we can use for research to production. . If you would like to study the tutorials from the offical pytorch website, please visit this link. The source code for the entire pytorch framework can be found here. . SideNote:Pytorch separates different tasks into different modules. e.g., there is a package called torchaudio that focus only on audio alone. . Today we will use torchvision, which is the package inside torch, that focus on vision tasks. For example, for data augmentation, torchvision provides the function called transforms. And if you would like to use transfer learning, torchvision also provides some of the state of the art pretrained models. . The torch.nn contains the basic building blocks that we need to construct our model. For example, if we need to construct a Convolutional Layer, we can call the function torch.nn.Conv2d() to construct that layer. . And if we want linear layer that perform the equation of $y = W^TX + b$, we can call the function torch.nn.Linear(). . If you would like to know more about torch.nn library, please visit this link for more information. Also, this post provides better understanding of torch.nn module. . import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import matplotlib.pyplot as plt import numpy as np . Main Components . Let&#39;s preview what will be built in this notebook. . Dataset cifar10 | . | Model Architecture Simple Model | . | Loss (Update model) Cross Entropy | . | Optimizer (Regularizer) Adam Optimizer | . | Metrics (Visual for User) Loss, Accuracy | . | Save Model Model Checkpoint | . | If you would like to change from CPU to GPU, select the Runtime --&gt; Change Runtime type and select GPU. . After done selecting, we can check whether we are running GPU or CPU by using the following function: . print(torch.cuda.is_available()) . True . device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) . cuda:0 . If the output show cuda:0, then the GPU is in used. . Dataset . In this notebook, we will use CIFAR10 dataset for classification. . CIFAR10 is a public classification dataset, which consists of 60,000 32 * 32 color images in 10 classes. There are 50,000 training images and 10,000 testing images. . In the following cell, we will do data augmentation for the dataset. When doing trasnforms on the test dataset, we only normalize the input, that is because we do not need to augment(change) our test dataset to see the result. . Tensor is just like Numpy&#39;s ndarray, except that it can do calculations on GPU, and is modified to fit the training neural network procedure. . Normalization equation: . $X = frac{X- mu}{ sigma}$. . There are many augmentation methods available, if you would like to know more, please visit this post. . class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;] # Image Normalization, Data Augmentation transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), transforms.RandomRotation(30) ]) test_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) train_dataset = torchvision.datasets.CIFAR10( &#39;./data&#39;, train=True, transform=transform, download=True) test_dataset = torchvision.datasets.CIFAR10( &#39;./data&#39;, train=False, transform=test_transform, download=True ) . Files already downloaded and verified Files already downloaded and verified . . Note: Pytorch stores the image dataset in the following format: batch_size * dim * height * width (B*D*H*W). . batch_size means number of images for one training iteration. . dim means the image dimension. Conventionally, color images have the dimension of 3 and gray scale images have the dimension of 1. . The label value for the dataset ranges from 0~9. e.g., if the label is 7, then, it would be class_names[7] = horse. . # Check the total number of images in train_dataset # Plot training Image and print label . Dataset and DataLoader . Dataset plays the huge role in machine learning and deep learning. For the computer vision task, we can do data augmentation to get the effect of regulating the model, avoid being overfitting. And when training, the augmented dataset needs to be fetched by the data loader. The main duty of the dataloader is to prepare dataset before feeding into the neural network. . Dataloader object is a generator object, which can be accessed through iteration. . train_loader = torch.utils.data.DataLoader(train_dataset, 32, shuffle=True, num_workers=2) test_loader = torch.utils.data.DataLoader(test_dataset, 8, shuffle=True, num_workers=2) . # Explain Generator # Access train_loader . Model . Let&#39;s build a simple Convolutional Neural Network. . When flattening the network, we can use the equation: . $O = frac{W-K+2P}{S} + 1$ . class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.conv1 = nn.Conv2d(3, 64, 3) self.conv2 = nn.Conv2d(64, 256, 3) self.conv3 = nn.Conv2d(256, 256, 3) self.conv4 = nn.Conv2d(256, 128, 3) self.fc1 = nn.Linear(24*24*128, 512) self.fc2 = nn.Linear(512, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.relu(self.conv2(x)) x = F.relu(self.conv3(x)) x = F.relu(self.conv4(x)) x = x.view(-1, 24*24*128) x = F.relu(self.fc1(x)) x = self.fc2(x) return x . model = SampleModel() # Move model to GPU model.to(device) # Define Loss criterion = nn.CrossEntropyLoss() # Define Optimizer optimizer = optim.Adam(model.parameters()) # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) . # Test Model . EPOCH = 2 loss_log = [] acc_log = [] x_coor = [] for epoch in range(EPOCH): loss_epoch = 0 total_imgs = 0 correct_epoch = 0 for i, data in enumerate(train_loader): # Get Image data and Label data imgs, labels = data[0].to(device), data[1].to(device) # Clear out the gradient optimizer.zero_grad() # Forward Propagation Start # Predict from Input : B * 10 predicts = model(imgs) # Calculate Loss from batch : 1 loss = criterion(predicts, labels) # Forward Propagation End # Backward Propagation Start # Calculate Gradient loss.backward() # Update Model parameters with optimizer : Adam or SGD optimizer.step() # Backward Propagation End # Add to Epoch Loss loss_epoch += loss.item() # Total Number of images in one batch total_imgs += len(imgs) # Count the total number of correct prediction correct_batch = (torch.argmax(predicts, 1)==labels).sum().item() correct_epoch += correct_batch acc_batch = correct_batch/ len(imgs) # Adding to tensorboard x_coor.append((i*len(imgs))+(epoch*len(train_dataset))) loss_log.append(loss.item()) acc_log.append(acc_batch) acc_epoch = (correct_epoch/total_imgs)*100 loss_epoch = loss_epoch/total_imgs print(f&quot;EPOCH : {epoch+1}, Acc : {acc_epoch:.2f}, Loss : {loss_epoch:.2f}&quot;) . EPOCH : 1, Acc : 40.23, Loss : 0.05 EPOCH : 2, Acc : 53.20, Loss : 0.04 . plt.plot(x_coor, loss_log, label = &#39;training_loss&#39;) plt.legend() plt.show() plt.plot(x_coor, acc_log, label = &#39;training_acc&#39;) plt.legend() plt.show() . Tensorboard . Tensorboard provides the visualization and tooling needed for machine learning experimentation. After doing research or wanting to put the model to production, we want graph showing the performance result of our model. Tensorboard offer logging the training loss and accuracy, visualizing the model and much more. You could also visit this link for more information. . model = SampleModel().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters()) # Import tensorboard library from torch.utils.tensorboard import SummaryWriter EPOCH = 2 # This create a folder where the logging will be stored in. writer = SummaryWriter(&#39;log&#39;) for epoch in range(EPOCH): loss_epoch = 0 total_imgs = 0 correct_epoch = 0 for i, data in enumerate(train_loader): # Get Image data and Label data imgs, labels = data[0].to(device), data[1].to(device) # Clear out the gradient optimizer.zero_grad() # Forward Propagation Start # Predict from Input : B * 10 predicts = model(imgs) # Calculate Loss from batch : 1 loss = criterion(predicts, labels) # Forward Propagation End # Backward Propagation Start # Calculate Gradient loss.backward() # Update Model parameters with optimizer : Adam or SGD optimizer.step() # Backward Propagation End # Add to Epoch Loss loss_epoch += loss.item() # Total Number of images in one batch total_imgs += len(imgs) # Count the total number of correct prediction correct_batch = (torch.argmax(predicts, 1)==labels).sum().item() correct_epoch += correct_batch acc_batch = correct_batch/ len(imgs) # Adding to tensorboard writer.add_scalar(&#39;Loss/Train&#39;, loss.item(), (i*len(imgs))+(epoch*len(train_dataset))) writer.add_scalar(&#39;Accuracy/Train&#39;, acc_batch, (i*len(imgs))+(epoch*len(train_dataset))) acc_epoch = (correct_epoch/total_imgs)*100 loss_epoch = loss_epoch/total_imgs print(f&quot;EPOCH : {epoch+1}, Acc : {acc_epoch:.2f}, Loss : {loss_epoch:.2f}&quot;) writer.add_graph(model, imgs) . EPOCH : 1, Acc : 35.37, Loss : 0.06 EPOCH : 2, Acc : 49.79, Loss : 0.04 . %load_ext tensorboard %tensorboard --logdir log . Output hidden; open in https://colab.research.google.com to view. . Hyperparameter Tunning . Let&#39;s do a simple tunning process where we will look at how learning rate will effect the model performace. . Below, we select three learning rate value : 0.1, 0.01 and 0.001, and plot their respective result on the tensorboard. . from torch.utils.tensorboard import SummaryWriter EPOCH = 2 learning_rate = [0.1, 0.01, 0.001] for lr in learning_rate: # Let&#39;s restart the training model = SampleModel().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=lr) writer = SummaryWriter(comment = f&quot;lr_{lr}&quot;) for epoch in range(EPOCH): loss_epoch = 0 total_imgs = 0 correct_epoch = 0 for i, data in enumerate(train_loader): # Get Image data and Label data imgs, labels = data[0].to(device), data[1].to(device) # Clear out the gradient optimizer.zero_grad() # Forward Propagation Start # Predict from Input : B * 10 predicts = model(imgs) # Calculate Loss from batch : 1 loss = criterion(predicts, labels) # Forward Propagation End # Backward Propagation Start # Calculate Gradient loss.backward() # Update Model parameters with optimizer : Adam or SGD optimizer.step() # Backward Propagation End # Add to Epoch Loss loss_epoch += loss.item() # Total Number of images in one batch total_imgs += len(imgs) # Count the total number of correct prediction correct_batch = (torch.argmax(predicts, 1)==labels).sum().item() correct_epoch += correct_batch acc_batch = correct_batch/ len(imgs) # Adding to tensorboard writer.add_scalar(&#39;Loss/Train&#39;, loss.item(), (i*len(imgs))+(epoch*len(train_dataset))) writer.add_scalar(&#39;Accuracy/Train&#39;, acc_batch, (i*len(imgs))+(epoch*len(train_dataset))) acc_epoch = (correct_epoch/total_imgs)*100 loss_epoch = loss_epoch/total_imgs print(f&quot;LR : {lr}, EPOCH : {epoch+1}, Acc : {acc_epoch:.2f}, Loss : {loss_epoch:.2f}&quot;) . EPOCH : 1, Acc : 9.87, Loss : 8932.46 EPOCH : 2, Acc : 9.89, Loss : 0.07 EPOCH : 1, Acc : 10.12, Loss : 0.28 EPOCH : 2, Acc : 9.88, Loss : 0.07 EPOCH : 1, Acc : 42.73, Loss : 0.05 EPOCH : 2, Acc : 55.18, Loss : 0.04 . %load_ext tensorboard %tensorboard --logdir runs . Output hidden; open in https://colab.research.google.com to view. . !zip -r /content/runs.zip /content/runs . Saving Model . ckpt_path = &#39;checkpoint.pt&#39; # State Difference between model.parameters and model.state_dict() torch.save(model.state_dict(), ckpt_path) . model = SampleModel().to(device) checkpoint = torch.load(ckpt_path) model.load_state_dict(checkpoint) . &lt;All keys matched successfully&gt; . !cp . cp: missing file operand Try &#39;cp --help&#39; for more information. . num_total = 0 num_correct = 0 # Deactivate Drop out and Batch-Normalization layers. model.eval() # Do not store gradient info in forward propagation. with torch.no_grad(): for i, data in enumerate(test_loader): image, label = data image = image.to(device) label = label.to(device) predict = model(image) predict_class = torch.argmax(predict, dim=1) correct = (predict_class == label) num_correct += correct.sum().item() num_total += len(image) print(num_correct, num_total) . 6134 10000 . print(&quot;Accuracy : &quot;, num_correct/num_total) . Accuracy : 0.6134 . def test_model(model): data = next(iter(test_loader)) imgs, labels = data[0].to(device), data[1] predicts = model(imgs) print(predicts.size()) index = torch.argmax(predicts, dim=1) titles = [] for i in index: titles.append(class_names[i]) plt.figure(figsize=(20,10)) for i in range(len(titles)): title = f&quot;Predict : {titles[i]}, Actual : {class_names[labels[i]]}&quot; if titles[i]==class_names[labels[i]]:color = &#39;blue&#39; else:color=&#39;red&#39; img = (imgs[i].cpu().numpy()/2)+0.5 plt.subplot(2, 4, i+1) plt.imshow(img.transpose(1, 2, 0));plt.title(title, fontdict={&#39;fontsize&#39;:17, &#39;color&#39;:color}) test_model(model) . torch.Size([8, 10]) . # !conda install tensorboard . Collecting package metadata (current_repodata.json): done Solving environment: done # All requested packages already installed. . # %load_ext tensorboard # %tensorboard --logdir runs . # print(len(train_dataset)) # # img_ind ~ 0~4,999 # # # img_ind = 60 # image = train_dataset[img_ind][0].numpy() # label = train_dataset[img_ind][1] # image = image.transpose(1, 2, 0) # print(class_names[label]) # plt.imshow(image) # plt.show() . # # Explain Generator # # Access train_loader # for data in train_loader: # print(data[0].size()) # print(data[1].size()) # break . # # Test Model # input_images = torch.rand(1, 3, 32, 32).to(device) # prediction = model(input_images) # prediction .",
            "url": "https://ytu-cvlab.github.io/mce-51069/pytorch/convolutional_neural_network/2020/12/21/week2-day3.html",
            "relUrl": "/pytorch/convolutional_neural_network/2020/12/21/week2-day3.html",
            "date": " • Dec 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Week 2, Day 1 (Introduction to Artificial Intelligence and Computer Vision)",
            "content": "Artificial Intelligence . Human has always been fascinated by the ideas of putting the intelligence to the machine. Still, it was until 1956, the term &quot;Artificial Intelligence&quot; was coined, at a conference at Dartmouth College, in Hanover, New Hampshire. At such time, people were very optimistic about the future of AI. And the fundings and interest invest in the field get larger and larger. But making a machine to behave just like a human is not an easy task. With the researchers fail to deliver the promise, and also with several reports criticizing progress in AI, the funding and interest in the field get dropped off, people later often refers this as the &quot;AI winter&quot; which happend during 1974s-1980. . Even though there are some private and public fundings to the field, the whole hype about &quot;Aritificial Intelligence&quot; gets cooled down. And around 1987s to 1993, the field experienced another &quot;winter&quot;. . It was only after 1997, IBM&#39;s Deep Blue became the first computer to beat a chess champion, Russian grandmaster Garry Kasparov, that the term &quot;AI&quot; is coming back to the reserach ground. . The field of artificial intelligence finally had its breakthrough moment in 2012 at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) with the introduction of Alex-Net. (ILSVRV) is a vision competition and before Alex-Net, the error rate hover around 26%. But with Alex-Net, the error rate comes down to only 16.4%. That is a huge accomplishment and people now see hope in Deep Neural Networks Again. . Nowaday, when people talk about &quot;Artificial Intelligence&quot;, they often refers to &quot;Deep Neural Networks&quot;, a branch of Machine learning that imitates the human brain cells as to function and process data. And with modern hardware advanced, big data accessable and algorithms optimized, the field of Deep Learning is getting more and more research interestes and improvements everyday. . Computer Vision . The main purpose of computer vision is for computer to have the &quot;vision&quot;, ability to perceive the world. To do that, we have to know how human vision system work. It is now known that, human vision is hierarchical. Neurons detect simple features like edges, then feed into more complex features like shapes, and then eventually feed into more complex visual representations. . Today, we have the AI algorithms, that mimic the hierarchical architecture of the human vision system called the CNN(Convolutional Neural Networks). CNNs use convolutional layers to extract the features in images and then use fully connected layers for the output. . . AI Available Today . Even though many people think that AI is far away from our daily life, it is not quite true. When we scroll on social Media, let&#39;s say Facebook, we get the newsfeed from Facebook&#39;s recommandation AI. And when we travel using &quot;Google Maps&quot;, it automatically generates the traffic conditions using AI. What we didn&#39;t realize is, AI has already been a part of our daily lives and improving our life quality. . Neural Network . Neural Network is the basic building block for the Deep Neural Networks. . Let&#39;s first do a simple classification on two data points. Given the x-coordinate of the data point, we will have to classify wherether this point belongs to red or blue. For that, we will need to find the threshold value, decision boundary or decision surface, whatever you may call it. For this particular example, the threshold value would be 2.5. . So, we would write this with the logic of taking x-coordinate as the input,x, and check if it is smaller than 2.5, if True, then, the color is &quot;Red&quot;, and if False, the color is &quot;Blue&quot;. . . We can also write the function instead of logic with x-2.5, and check if positive or negative. . But, still, we do not know the confidence of prediction value by the above equation. For such purposes, Sigmoid function is introduced. It keeps the output range from 0~1. So, if the output from sigmoid is near 1, let&#39;s say, 0.9, we would say that this point is 90% sure to be the &quot;Blue&quot; Point. And if the output from the sigmoid is 0.2, we would say this point is 20% sure to be the &quot;Blue&quot; Point or (1-0.2 = 0.8) 80% sure to be the &quot;Red&quot; Point. . . . Let&#39;s apply this logic with 2 dimensions(x and y). Remember when we introduced the threshold value, we refered the point as the decision boundary or decision surface. That is because that value(threshold) makes the decision for the model in a single dimension. For 2 dimensional cases, we will need a Line (2D) instead of Point (1D) as a decision boundary for classification. . So, as the dimension for the dataset increases, the dimension of the decision surface also increases. In 1D : $(x-2.5)$ : Only 1 variable In 2D : $(0.5x -y -1)$ : 2 variables . For this particular example, let&#39;s say, the equation for the line is $ y = 0.5x - 1$, then, it can be derived to $ 0.5x - y - 1$. Here, we introduce &quot;Weight&quot;, the coefficient of variables, and &quot;Bias&quot;, which is a constant value. . And with the extra loss function, the basic logistic regression model is developed. Loss function for the model determines how badly the model is performing. The loss function is also known as &quot;Binary cross-entropy&quot; Loss. . . After finding the Loss of the model, We would like to Update(Optimize) the model for better classification performance. To update the model, gradient descent method is introduced. . First, the variables(parameters) that we can update in the model are &quot;weight&quot; and &quot;bias&quot;. So, we need to know, how the change in weight effect loss. Mathematically, this is called the derivative. So, we would like to know $ frac{dL}{dW}$. . And since the equation for the loss is: $L = -[ylog hat{y} + (1-y)log(1- hat{y})]$, L depends on $ hat{y}$ $ hat{y} = sigma{z}$, $ hat{y}$ depends on z $Z = W^TX + B$, Z depands on W, . We can use chain Rule to find the derivative of L and W with the following equation. . $ frac{dL}{dW} = (y - hat{y}) * X$ . If you want to know the exact steps of derivation, here is this link from stack exchange. . After that, we can update the weight values with gradient descent equation: $W = W - alpha frac{dL}{dW}$ $ alpha$ here means the learning rate, which is a hyperparameter we can adjust to get the better convergence rate for the model. . . Forward and Backward Propagation . Forward propagation is when the data points pass through the model and output the loss value. Backward propagation happen to update the model parameters(W) with the loss or cost, accumulation of loss value gathered during the forward propagation. . . The reason why this is called the gradient descent is becasue $ frac{dL}{dW}$ will approach to zero, minimal value, while updating the model parameter. . . Deep Neural Network . What happens if we add more units of cells to the previous model? . The main problem with just simply stacking cells is that the model is still acting linearly. So, we need to introduce some kind of non-linearity to the model, by adding &quot;activation function&quot; to each output of the cell. . . Now, we will introduce Relu (Rectified Linear Unit), which behaves just like linear function if the output is higher than 0. . The equation for the relu function is as follow: . $relu(z) = begin{cases} z &amp; if z&gt;0 0 &amp; if z&lt;=0 end{cases}$ . And the derivative for the relu is $ frac{d}{dz}relu(z) = begin{cases} 1 &amp; if z&gt;0 0 &amp; if z&lt;=0 end{cases}$ . The main advantage of using relu is that it does not activate all the neurons at the same time. When the value of z is negative, relu turns it off, returning the zero value, indicating that this feature is not important for the neurons to learn. And since only a certain number of neurons get activated, it is far more computationally efficient than sigmoid and other activation functions. . If you would like to learn more about various Activation functions, please visit this blog post for more details. . Recap . To conclude of what we have learned, we need several things to construct a neural network: &quot;Weights and Bias&quot;, &quot;Activation Functions&quot; and &quot;Loss Function&quot;. . To update(learning) the model, the optimization method &quot;Gradient Descent&quot; was used. . Further Resources . If you would like to learn more about neural networks, you can visit Deeplearning.ai on youtube. .",
            "url": "https://ytu-cvlab.github.io/mce-51069/deep_learning/computer_vision/2020/12/19/week2-day1.html",
            "relUrl": "/deep_learning/computer_vision/2020/12/19/week2-day1.html",
            "date": " • Dec 19, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Week 1, Day 3 (Introduction to Image Processing in Python)",
            "content": "Notebooks and Assignments . Please download all the lectures and assignment notebooks of week1 (Day 3) here. We have also posted a guide video on downloading and accessing materials on youtube channel. . Numpy . Numpy is a strong third-party library that emphasizes numerical calculations in python. . In short, NumPy has a ndArray data type, which can process a large number of numbers faster and more efficiently than List. . The Basics . Arrays Creation . First, we will have to import numpy library. In python, it is . import numpy as np . Here, np is a convention that abbreviates numpy. So that, later in the program, we just need to type np to call numpy library. . There are many ways to create a numpy array. First, let&#39;s create an array from list. . array = np.array(list) . # import numpy library import numpy as np list_object = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]] # Let&#39;s Create an array array = np.array(list_object) # Print Out the Array print(list_object) print(array) print(type(array)) . . [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]] [[ 1 2 3 4 5] [ 6 7 8 9 10]] &lt;class &#39;numpy.ndarray&#39;&gt; . In the list, printing the object separates each element with a comma whilst in an array, it separates each one with space. . The Attributes for ndarray: . # The attributes in `ndarray`. # Check dtype of the array print(array.dtype) # Check item size (number of Bytes) print(array.itemsize) # Check array Size [Number of elements in that array] print(array.size) # Check number of axis print(array.ndim) # Check shape of array print(array.shape) # Check the byte of each element print(array.nbytes) . . int64 8 10 2 (2, 5) 80 . We can also create arrays from range. np.arange function is just like range() from python built-in functions. . np.arange([start,] stop[, step,], dtype=None) . The square bracket &#39;[ ]&#39; here means it is the optional argument and it has the default value set up. If we do not specify the value, it will take the default value. . e.g. the default value for start argument is 0. . # Let&#39;s create another array with function array = np.arange(20) print(array) array = np.arange(2, 20, 2) print(array) # start and stop same array = np.arange(2, 2, 2) print(array) # Reverse array = np.arange(40, 20, -2) print(array) # Float points array = np.arange(0, 2, 0.3) print(array) . . [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19] [ 2 4 6 8 10 12 14 16 18] [] [40 38 36 34 32 30 28 26 24 22] [0. 0.3 0.6 0.9 1.2 1.5 1.8] . But, with np.arange, we could not control the numbers of element in that array. But with np.linspace, it can be achieved. . array = np.linspace(start, stop, num=50) . Here, the num is set to default with 50, but we could alos specify the numbers of element that we want between the range. . # Create with the count of array wanted array = np.linspace(0, 2, 5) print(array) # reverse array = np.linspace(20, 2, 20) print(array.shape) print(array) # start and end same array = np.linspace(20, 20, 5) print(array) . . [0. 0.5 1. 1.5 2. ] (20,) [20. 19.05263158 18.10526316 17.15789474 16.21052632 15.26315789 14.31578947 13.36842105 12.42105263 11.47368421 10.52631579 9.57894737 8.63157895 7.68421053 6.73684211 5.78947368 4.84210526 3.89473684 2.94736842 2. ] [20. 20. 20. 20. 20.] . We can also initialize arrays by just describing the shape of the array that we want. . zeros = np.zeros(shape, dtype, order=&#39;C&#39;) ones = np.ones(shape, dtype, order=&#39;C&#39;) empty = np.empty(shape, dtype, order=&#39;C&#39;) . # Initialize array for place holder array = np.zeros((5,5)) print(array) print(array.dtype) . . [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] float64 . The default data type, when creating such array, is float64. But if we want to specify the data type of the array, we can do it as follows: . import numpy as np # Set data type to `uint8` array = np.zeros((3,3), dtype=np.uint8) print(array) print(array.dtype) . . [[0 0 0] [0 0 0] [0 0 0]] uint8 . np.ones() and np.empty() also initialize arrays like np.zeros(), but with different values. . array = np.ones((5,5)) print(array) array = np.empty((5,5)) print(array) . . [[1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.]] [[1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.]] . We can also create arrays from the existing list and set object by using np.array() function. Numpy can automatically choose the data type and set the data type for the array as either int64 or float64. . # Create with existing Variable nested_list = [[1, 2, 3], [4, 5, 6]] nested_array = np.array(nested_list) print(nested_list, &#39; n&#39;, nested_array) print(nested_array.dtype) nested_set = ((1, 2, 3), (4, 5, 6)) nested_array = np.array(nested_set) print(nested_set, &#39; n&#39;, nested_array) print(nested_array.dtype) # If we take in Float value, the array will recognize it as float64. list_var = [1., 2., 3.] array = np.array(list_var) print(type(list_var[0]), array.dtype) print(list_var, array) # But we can also specify the data type in `dtype` argument. list_var = [1., 2., 3.] array = np.array(list_var, dtype=np.uint8) print(type(list_var[0]), array.dtype) print(list_var, array) . . [[1, 2, 3], [4, 5, 6]] [[1 2 3] [4 5 6]] int64 ((1, 2, 3), (4, 5, 6)) [[1 2 3] [4 5 6]] int64 &lt;class &#39;float&#39;&gt; float64 [1.0, 2.0, 3.0] [1. 2. 3.] &lt;class &#39;float&#39;&gt; uint8 [1.0, 2.0, 3.0] [1 2 3] . . Basic Operations . # Simple Arithmetic Operations array_a = np.array([1, 2, 3]) array_b = np.array([4, 5, 6]) print(&quot;Addition of two array : t&quot;, array_a + array_b) print(&quot;Subtraction of two array : t&quot;, array_a - array_b) print(&quot;Multiplication of two array : t&quot;, array_a * array_b) print(&quot;Division of two array : t&quot;, array_a / array_b) . . Addition of two array : [5 7 9] Subtraction of two array : [-3 -3 -3] Multiplication of two array : [ 4 10 18] Division of two array : [0.25 0.4 0.5 ] . There are also many scientific functions built-in in numpy. These functions are called universal functions. . x = np.linspace(1, 5, 20) sine_array = np.sin(x) cos_array = np.cos(x) exp_array = np.exp(x) log_array = np.log(x) . . The random function in Numpy is also strong. It has the collection of various random distributons built in, including: Uniform Distribution, Standard Normal Distribution and Gaussian Distribution. . # Uniform Distribution uniform_dist = np.random.rand(100) print(uniform_dist.shape) # Standard Normal Distribution standard_normal_dist = np.random.randn(100) print(standard_normal_dist.shape) # Gaussian Distribution gaussian_dist = np.random.normal(1, 2, 100) print(gaussian_dist.shape) . . (100,) (100,) (100,) . Indexing, Slicing and Iterating . 1D Arrays can be indexed, sliced and iterated over much like list. For the arrays of higher dimensions, we have to specify the index or range for each specific axes. . 1D Array . a = np.arange(0, 20, 2) print(a) # Index : Select sepecific element element = a[1] print(element) # Slicing : Select Range of element range_element = a[1:5] print(range_element) # Reverse Slicing reverse_element = a[8:2:-1] print(reverse_element) # Iteration for i in a: print(i, end=&#39; &#39;) . . [ 0 2 4 6 8 10 12 14 16 18] 2 [2 4 6 8] [16 14 12 10 8 6] 0 2 4 6 8 10 12 14 16 18 . 2D and higher Dimension Arrays . # 2D array array = np.array([[1, 2, 3], [4, 5, 6]]) print(array.shape) print(array) # Indexing first_element = array[0, 0] last_element = array[-1, -1] print(first_element, last_element) # Range print(array) first_row = array[0, :] last_column = array[:, -1] print(first_row, last_column) . . (2, 3) [[1 2 3] [4 5 6]] 1 6 [[1 2 3] [4 5 6]] [1 2 3] [3 6] . # 3D array array = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]]]) print(array) print(array.shape) # Indexing first_element = array[0, 0, 0] # First_element last_element = array[2, 1, 2] # Last_element print(first_element, last_element) # Range print(array) red_channel = array[:, :, 0] green_channel = array[:, :, 1] blue_channel = array[:, :, 2] print() print(red_channel, green_channel, blue_channel) . . [[[ 1 2 3] [ 4 5 6]] [[ 7 8 9] [10 11 12]] [[13 14 15] [16 17 18]]] (3, 2, 3) 1 18 [[[ 1 2 3] [ 4 5 6]] [[ 7 8 9] [10 11 12]] [[13 14 15] [16 17 18]]] [[ 1 4] [ 7 10] [13 16]] [[ 2 5] [ 8 11] [14 17]] [[ 3 6] [ 9 12] [15 18]] . . Arrays Manipulation . Numpy has built-in functions to manipulate the shape of the array. By using these methods, the shape attribute of the array can be changed. . . array.reshape function is often used to reshape the original array into desired shape. . Note: When reshaping the array, the total size of the array cannot be changed. e.g., an array having the shape of (3,3) cannot be reshaped into (4,3). It will throw ValueError: cannot reshape array of size 8 into shape (3,3) . Tips:When you are only sure about one dimension for the array, you can use -1 for the rest of array dimensions to let numpy automatically choose the rest. . array = np.zeros((2, 4)) print(array.shape) reshaped = array.reshape((4, 2)) print(reshaped.shape) # Here we only know shape for one dimension unknown_shape = array.reshape((2, -1)) print(unknown_shape.shape) . . (2, 4) (4, 2) (2, 4) . So the array automatically figures out the rest of the shape in the dimension. . You can use numpy built-in functions to stack arrays. There are three stacking functions dstack, hstack and vstack. Please refer to the above animation for more understanding of stacking arrays. . # This method stack arrays in third axes dstack = np.dstack((array, array)) print(dstack.shape) # This method stack arrays in second axes hstack = np.hstack((array, array)) print(hstack.shape) # This method stack arrays in first axes vstack = np.vstack((array, array)) print(vstack.shape) . . (2, 4, 2) (2, 8) (4, 4) . . Ordering . In numpy, we can find the minimum and maximum of the array by simply calling built-in functions np.min(array) and np.max(array). Also, the index of these values can be found by usingnp.argmin(array) and np.argmax(array). . . array = np.array([2, 1, 2, 5, 2, 100, 2, 99, 12]) # we can sort the array by sorted_array = np.sort(array) # Finding the min, max, argmin, argmax value is easy in numpy by: min_value = np.min(array) argmin = np.argmin(array) max_value = np.max(array) argmax = np.argmax(array) print(&quot;Original Array : t t t&quot;, array) print(&quot;Sorted Array : t t t t&quot;, sorted_array) print(&quot;Minimum value in Array : t t&quot;, min_value) print(&quot;Index where minimum value exists : t&quot;, argmin) print(&quot;Maximum value in Array : t t&quot;, max_value) print(&quot;Index where maximum value exists : t&quot;, argmax) . . Original Array : [ 2 1 2 5 2 100 2 99 12] Sorted Array : [ 1 2 2 2 2 5 12 99 100] Minimum value in Array : 1 Index where minimum value exists : 1 Maximum value in Array : 100 Index where maximum value exists : 5 . . Basic Statistics . We can find the basic statistics values Mean, Standard Deviation and Variance in the array by using the numpy built-in functions. . array = np.arange(20) print(array) mean = np.mean(array) std = np.std(array) var = np.var(array) print(f&quot;Mean : t t t{mean} nStandard Deviation : t{std:.2f} nVariance : t t{var}&quot;) . . Although the above functions are built-in in numpy library, we can also implement them using equations: . mean = np.sum(array)/array.size print(mean) std = np.sqrt(np.sum((array-mean)**2)/array.size) print(std) var = np.sum((array-mean)**2)/(array.size-1) print(var) . . 9.5 5.766281297335398 35.0 . . Simple Comparison . Let&#39;s compare the calculation time needed between List and ndarray. . # Let&#39;s try to compare for large numbers. array_obj = np.arange(1000000) print(array_obj.shape) print(type(array_obj)) # We can convert from ndarray object to list object with list_obj = [i for i in range(1000000)] print(len(list_obj)) print(type(list_obj)) . . (1000000,) &lt;class &#39;numpy.ndarray&#39;&gt; 1000000 &lt;class &#39;list&#39;&gt; . . Code Complexity . Add 1 to each element in array and list. Here, we can see that list has to use For loop to iterate over each element whilst arrays in numpy carries out using numpy broadcasting. . list_result = [i+1 for i in list_obj] # add 1 to array array_obj = array_obj + 1 . The above mentioned is the simplest one, where the dimension is only 1. Let&#39;s try with higher dimensions. . # let&#39;s create 3d arrays array_obj = np.arange(100000).reshape(10,10,-1) print(type(array_obj)) # We could convert array to list by: list_obj = array_obj.tolist() print(type(list_obj)) # Add 1: to list for i in range(len(list_obj)): for j in range(len(list_obj[0])): for k in range(len(list_obj[0][0])): list_obj[i][j][k] += 1 # Add 1: to Array array_obj += 1 . . &lt;class &#39;numpy.ndarray&#39;&gt; &lt;class &#39;list&#39;&gt; . . Speed . %%timeit array_obj = np.arange(1000000) array_obj = array_obj + 1 . . 3.24 ms ± 101 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . %%timeit list_obj = [i for i in range(1000000)] list_obj = [i+1 for i in range(1000000)] . . 204 ms ± 58.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . We can see that doing one operation in creating an array is around 60 times faster than the list in the above comparison. As the size of the element and operations increase more, the difference will go higher. . Further Resources for Numpy . If you wanna know more about Numpy arrays, please visit this official Numpy documentation. You can also learn more about Numpy arrays in this blog post. . Matplotlib . Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. The following plots are referenced from matplotlib&#39;s official website For this lecture, we will use matplotlib.pyplot which is a collection of functions that make matplotlib work like MATLAB. . Simple Plots . First, we need to know how the pyplot functions work. Let&#39;s start with some basic functions. . %matplotlib inline import matplotlib.pyplot as plt import numpy as np . . x = np.linspace(0, 10, 100) plt.plot(x,x) . . [&lt;matplotlib.lines.Line2D at 0x1993e556c88&gt;] . plt.plot(np.arange(8),[0, 1, 2, 3,3.5,4,4.5,5]) plt.xlabel(&#39;This is x label&#39;) plt.ylabel(&#39;This is y label&#39;) plt.show() . . x = np.linspace(0, 10, 100) fig = plt.figure() plt.plot(x, np.sin(x), &#39;-&#39;) plt.plot(x, np.cos(x), &#39;--&#39;); plt.title(&quot;Sine and Cosine waveforms&quot;) plt.savefig(&quot;sine_cos_wave.png&quot;) . . Here, we have mentioned plt.plot() plt.xlabel(),plt.ylabel() and plt.title(). Let&#39;s look at more examples. . x = np.arange(10) plt.plot(x,x**2,linestyle=&#39;dashed&#39;,linewidth=2, markersize=12) plt.xlabel(&quot;number&quot;) plt.ylabel(&quot;number squared&quot;) . . Text(0, 0.5, &#39;number squared&#39;) . Now,we have learnt that additional customizations can be added to the plots with use of some additional arguments. Also, we can make multiple plots on the same plane. . import numpy as np x = np.arange(10) plt.plot(x,x**2,x,3*x,&#39;r+&#39;,x,7*x,&#39;bo&#39;) . . [&lt;matplotlib.lines.Line2D at 0x199518f49c8&gt;, &lt;matplotlib.lines.Line2D at 0x1994eef0a88&gt;, &lt;matplotlib.lines.Line2D at 0x199518f4d08&gt;] . x = np.linspace(0, 10, 100) plt.plot(x, x + 0, &#39;-g&#39;) # solid green plt.plot(x, x + 1, &#39;--c&#39;) # dashed cyan plt.plot(x, x + 2, &#39;-.k&#39;) # dashdot black plt.plot(x, x + 3, &#39;:r&#39;); # dotted red . . Now that we have learnt how to manipulate plt.plot(), this should be enough for the assignments. If you want more information regarding this function, you can check their official documentation and also follow matplotlib&#39;s tutorial for this function. . Using Subplots . plt.subplot()is a handy function to display multiple plots. Let&#39;s dive in and see how it works . x1 = np.linspace(0.0, 5.0) x2 = np.linspace(0.0, 2.0) y1 = np.cos(2 * np.pi * x1) * np.exp(-x1) y2 = np.cos(2 * np.pi * x2) plt.subplot(2, 1, 1) plt.plot(x1, y1, &#39;o-&#39;) plt.title(&#39;A tale of 2 subplots&#39;) plt.ylabel(&#39;Damped oscillation&#39;) plt.subplot(2, 1, 2) plt.plot(x2, y2, &#39;.-&#39;) plt.xlabel(&#39;time (s)&#39;) plt.ylabel(&#39;Undamped&#39;) plt.show() . . This is the traditional method to create multiple plots. However, there is an alternative method which is more optimal for creating waveforms. . x1 = np.linspace(0.0, 5.0) x2 = np.linspace(0.0, 2.0) x3 = np.linspace(0.0, 5.0) y1 = np.cos(2 * np.pi * x1) * np.exp(-x1) y2 = np.cos(2 * np.pi * x2) y3 = np.sin(2 * np.pi * x3) * np.exp(-x1) fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20,10)) fig.suptitle(&#39;A tale of 3 subplots&#39;) ax1.plot(x1, y1, &#39;o-&#39;) ax1.set_ylabel(&#39;Damped oscillation&#39;) ax2.plot(x2, y2, &#39;.-&#39;) ax2.set_xlabel(&#39;time (s)&#39;) ax2.set_ylabel(&#39;Undamped&#39;) ax3.plot(x3, y3, &#39;o-&#39;) ax3.set_ylabel(&#39;Sine damped&#39;) plt.show() . . Form the code above, we now know that subplot function has input and basic output arguments: . figure, axes = plt.subplot(rows,columns,figure_size) . For more details about this function, please follow documentation and reference tutorial from matplotlib. . Scatter plots . A scatter plot uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables. Let&#39;s start with a simple scatter plot. . x = np.linspace(0, 10, 30) y = np.sin(x) plt.plot(x, y, &#39;o&#39;, color=&#39;black&#39;); . . Here, we simply used one of the plotstyles in plt.plot() to create a scatter plot. However, there is a better function for this purpose: plt.scatter() . N = 500 x = np.random.rand(N) y = np.random.rand(N) colors = (0,0,0) area = np.pi*3 # Plot plt.scatter(x, y, s=area, c=&quot;green&quot;, alpha=0.5) plt.title(&#39;Scatter plot&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.show() . . This should be enough for our course. If you want to learn more about creating different types of scatter plots, please follow this link . Histograms . The purpose of a Histogram is to graphically summarize the distribution of a single feature within a dataset. . x = np.random.randn(1000) plt.hist(x, color = &quot;red&quot;, edgecolor = &quot;black&quot;); plt.title(&quot;Histogram&quot;) . . Text(0.5, 1.0, &#39;Histogram&#39;) . plt.hist(x, bins=50, color=&#39;steelblue&#39;,edgecolor=&#39;black&#39;); . . Histograms are very useful when we make comparisons within datasets. For this example, we have gathered some data from galtron height dataset to compare the heights of 30 men and 30 women. . #Heights of 30 random women in inches w = [69.2,69,69,65.5,65.5,68,67,64.5,63,66.5,62.5,62.5,69.5,70.5,64,70.5,68,66,66,65.5,68,67,67,66,63.5,63,65,66.7,68.7,62] #Heights of 30 random men in inches m = [73.2,73.5,72.5,71,70.5,68.5,72,69,68,76.5,74,73,73,74,70,68,67,71,70.5,72,70.5,70.2,70.2,69.2,74,73,71.5,62.5,73.2,73] plt.hist(w, bins=10,color=&#39;red&#39;,edgecolor=&#39;black&#39;, alpha =0.5) plt.hist(m, bins=10,color=&#39;blue&#39;,edgecolor=&#39;black&#39;, alpha =0.5) plt.title(&quot;Height distributions of Men and Women&quot;) . . Text(0.5, 1.0, &#39;Height distributions of Men and Women&#39;) . From this Histogram, we can definitely reach a conclusion that men are usually taller than women. . references - matplotlib, python data science handbook . Further Resources for Matplotlib . If you want to learn more about matplotlib, please visit these official matplotlib tutorials and shorter examples. You can also refer to this github for more in depth tutorials. . OpenCV . OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications. . Before we start, Let&#39;s install opencv library in your PC. . !pip install opencv-python . Requirement already satisfied: opencv-python in c: users aw thura appdata roaming python python38 site-packages (4.2.0.34) Requirement already satisfied: numpy&gt;=1.17.3 in c: users aw thura appdata roaming python python38 site-packages (from opencv-python) (1.18.4) . Reading &amp; Writing Images . In opencv, we can read and write image files with imread() and imwrite() and display them using imshow() waitKey(0) waits until any key is pressed and waitKey(milliseconds) waits for a certain amount of time before the window is closed by destroyAllWindows() . %matplotlib inline import cv2 import numpy as np import matplotlib.pyplot as plt import matplotlib.image as mpimg . . img = cv2.imread(&quot;images/McE_logo.png&quot;,1) # img = cv2.imread(&quot;images/McE_logo.jpg&quot;,0) # img = cv2.imread(&quot;images/McE_logo.jpg&quot;,-1) # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.imshow(&quot;Original&quot;,img) cv2.waitKey(0) # cv2.waitKey(2000) cv2.destroyAllWindows() . . cv2.imread() reads images in BGR but matplotlib displays images in RGB format. Thus, we need to convert formats using cv2.cvtColor(). We can also use this function to convert images to grayscale. . #convert to BGR format for matplotlib # img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) plt.imshow(img) . . &lt;matplotlib.image.AxesImage at 0x19951c132c8&gt; . img = mpimg.imread(&quot;images/McE_logo.png&quot;) plt.imshow(img) . . &lt;matplotlib.image.AxesImage at 0x19951f4ffc8&gt; . img.fill() fills the entire image with particular shade of grayscale. . Adding Shapes &amp; Letters . We could do a bunch of stuff on our imported images. Let&#39;s try some functions. . img = cv2.imread(&quot;images/McE_logo.png&quot;,1) # cv2.line(image, (x1, y1), (x2, y2), (0, 255, 0), lineThickness) cv2.line(img, (0, 0), (512, 512), (0, 255, 0), 3, lineType=8, shift=0) #convert to RGB format for matplotlib img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) plt.imshow(img) . . &lt;matplotlib.image.AxesImage at 0x19952322348&gt; . #cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), lineThickness, LineType , shift) cv2.rectangle(img, (0,0), (500, 150), (123, 200, 98), 3, lineType=8, shift=1) plt.imshow(img) . . &lt;matplotlib.image.AxesImage at 0x19953c5f248&gt; . # cv2.circle(img, center, radius, color, thickness=1, lineType) cv2.circle(img, (300, 300), 70, (0, 0, 255)) plt.imshow(img) . . &lt;matplotlib.image.AxesImage at 0x19953cc5e88&gt; . font = cv2.FONT_HERSHEY_DUPLEX cv2.putText(img, &#39;McE&#39;, (80, 70), font, 3, (0,0,0), 2) plt.imshow(img) . . &lt;matplotlib.image.AxesImage at 0x19953d35648&gt; . Thus, after drawing a bunch of beautiful stuff on our images using cv2.line(), cv2.rectangle(), cv2.circle() and cv2.putText(), we might not want our hard work to go waste. Let&#39;s try and save the image. using cv2.imwrite() . # cv2.imwrite(path to saved image, img) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) cv2.imwrite(&quot;saved_image.jpg&quot;, img) . . True . Resizing images . Image resizing is an important function for machine learning. Most images are needed to be resized to match the model&#39;s configurations. It can also serve as an augmentation method. Let&#39;s see what cv2.resize() can do. . img = cv2.imread(&quot;images/PL-Lion.png&quot;) print(img.shape) . . (620, 929, 3) . resized_img = cv2.resize(img,(500,500)) print(resized_img.shape) . . (500, 500, 3) . plt.figure(figsize=(10,10)) plt.subplot(1, 2, 1) plt.imshow(img) plt.subplot(1, 2, 2) plt.imshow(resized_img) . . &lt;matplotlib.image.AxesImage at 0x199517c4108&gt; . Splitting &amp; Merging color channels . cv2.split() offers a simple way to extract color channels.This is very handy as it offers manipulation to the color channels. As an alternative, we can also slice images. The channels can be then remerged using cv2.merge(). . img=cv2.imread(&#39;images/lenaa.jpg&#39;) # b,g,r = cv2.split(img) b = img[:,:,0] g = img[:,:,1] r = img[:,:,2] hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV) h, s, v = cv2.split(hsv) print (s) # s.fill(255) # print (s) hsv = cv2.merge((h,s,v)) hsv = cv2.cvtColor(hsv,cv2.COLOR_HSV2RGB) original = cv2.merge((r,g,b)) mixed = cv2.merge((b,g,r)) stacked= np.hstack((hsv,mixed)) plt.figure(figsize=(8,8)) plt.xticks([]), plt.yticks([]) # remove the ticks plt.imshow(stacked) . . [[ 8 8 8 ... 12 12 12] [ 8 8 8 ... 13 12 12] [ 8 8 8 ... 13 13 13] ... [13 13 13 ... 15 15 15] [12 12 12 ... 15 15 15] [12 12 12 ... 15 15 15]] . &lt;matplotlib.image.AxesImage at 0x1994eef0588&gt; . Let&#39;s try displaying each channel subplot() instead of stacking . img=cv2.imread(&#39;images/lenaa.jpg&#39;) b,g,r = cv2.split(img) titles = [&#39;Original_Image&#39;, &#39;Blue Channel&#39;, &#39;Green Channel&#39;, &#39;Red Channel&#39;] img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) images = [img, b, g, r] plt.figure(figsize=(10,10)) for i in range(len(titles)): plt.subplot(2, 2, i+1) plt.imshow (images[i],&quot;gray&quot;) plt.title(titles[i]) plt.xticks([]), plt.yticks([]) plt.show() . . Image Transformations . Images can be translated and rotated with ease using cv2.wrapAffine(img,transformation matrix, resolution). Let&#39;s import our lab&#39;s logo first: . img = cv2.imread(&quot;images/cvml.png&quot;) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) plt.imshow(img) . . &lt;matplotlib.image.AxesImage at 0x199542b2988&gt; . To use cv2.wrapAffine() we must define our transformation matrix and resolution of the image. If we get our dimensions right, we can translate our image to anywhere on the figure. . h,w,ch = img.shape M = np.float32([[1,0,100],[0,1,50]]) translated = cv2.warpAffine(img,M,(w,h)) plt.xticks([]), plt.yticks([]) plt.imshow(translated) . . &lt;matplotlib.image.AxesImage at 0x19963091248&gt; . For rotational purposes, we could use cv2.getRotationMatrix2d(center,degrees,scale) to calculate our rotation matrix M. . h,w,ch = img.shape M = cv2.getRotationMatrix2D((w//2, h//2),45, 1) print(M) rotated = cv2.warpAffine(img, M, (w, h)) plt.xticks([]), plt.yticks([]) plt.imshow(rotated) . . [[ 0.70710678 0.70710678 -186.39610307] [ -0.70710678 0.70710678 450. ]] . &lt;matplotlib.image.AxesImage at 0x199630c98c8&gt; . For reflection (flipping) purposes, we can simply use cv2.flip(img,flipCode)to flip the image. . img = cv2.imread(&quot;images/MCE_logo.png&quot;) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) flipped = cv2.flip(img,-1) plt.xticks([]), plt.yticks([]) plt.imshow(flipped) . . &lt;matplotlib.image.AxesImage at 0x19963109b08&gt; . Edge Detection methods . Edge detection is an image processing technique for finding the boundaries of objects within images. There are several ways to perform edge detection and we will focus on two popular methods for this lecture. . . Sobel gradients . Sobel operators uses kernels) to calculate approximations of the derivatives.cv2.Sobel() needs image, kernel size and operating axis as input arguments. . We calculate x direction and y direction gradients first before calculating the magnitude of the sobel gradient. For detailed explaination on sobel gradients, you can follow this link. . img = mpimg.imread(&#39;images/McE_logo.png&#39;) img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) sobelx = np.abs(cv2.Sobel(img,5,1,0)) sobely = np.abs(cv2.Sobel(img,5,0,1)) #rescale x and y sobelx = sobelx/np.max(sobelx) sobely = sobely/np.max(sobely) #magnitude of sobel gradient sobel = np.sqrt(sobelx**2 + sobely**2) plt.figure(figsize=(10,10)) plt.subplot(2,2,1),plt.imshow(img,cmap = &#39;gray&#39;) plt.title(&#39;Original&#39;), plt.xticks([]), plt.yticks([]) plt.subplot(2,2,2),plt.imshow(sobelx,cmap = &#39;gray&#39;) plt.title(&#39;Sobel X&#39;), plt.xticks([]), plt.yticks([]) plt.subplot(2,2,3),plt.imshow(sobely,cmap = &#39;gray&#39;) plt.title(&#39;Sobel Y&#39;), plt.xticks([]), plt.yticks([]) plt.subplot(2,2,4),plt.imshow(sobel,cmap = &#39;gray&#39;) plt.title(&#39;Sobel&#39;), plt.xticks([]), plt.yticks([]) plt.show() . . . Canny edge detection . Canny Edge Detection is a popular edge detection algorithm developed by John F. Canny in 1986. cv2.Canny() has three arguments: Input image &amp; min and max values of hysteresis threshold. You can learn more about canny edge detection here. . img = cv2.imread(&#39;images/McE_logo.png&#39;,0) edges = cv2.Canny(img,100,200) plt.figure(figsize=(10,10)) plt.subplot(1,2,1),plt.imshow(img, cmap = &#39;gray&#39;) plt.title(&#39;Original Image&#39;), plt.xticks([]), plt.yticks([]) plt.subplot(1,2,2),plt.imshow(edges, cmap = &#39;gray&#39;) plt.title(&#39;Edge Image&#39;), plt.xticks([]), plt.yticks([]) plt.show() . . . Face Detection with CascadeClassifier . In 2001, Paul Viola and Michael Jones published a research paper called &quot;Rapid Object Detection using a Boosted Cascade of Simple Features&quot;. This paper sets the standards for new generations of face detectors computer vision field and still gets thousands of citations in 2020. Their face detection method is now known as Viola-Jones method. . detectMultiScale() from CascadeClassifier() returns 4 objects, namely, x and y coordinates of the detected face bottom left cornor and width and height of the detected face. . face_cascade = cv2.CascadeClassifier(&#39;detectors/haarcascade_frontalface_default.xml&#39;) # img = cv2.imread(&#39;images/lenaa.jpg&#39;) img = cv2.imread(&#39;images/poster.jpg&#39;) faces = face_cascade.detectMultiScale(img, 1.1, 5) print(faces) for (x, y, w, h) in faces: cv2.rectangle(img, (x,y), (x+w , y+h), (255, 0, 0), 3) # cv2.imshow(&quot;Detection&quot;,img) # cv2.waitKey(0) # cv2.destroyAllWindows() #convert to BGR format for matplotlib img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) plt.imshow(img) . . references -Opencv tutorials, Python data science handbook . Photo reference - Lena, Premier League . Further Resources for OpenCV . Please refer to official OpenCV Python tutorials to learn more about image processing techniques using cv2. Also check this awesome opencv tutorials from pyimageserach website. .",
            "url": "https://ytu-cvlab.github.io/mce-51069/image_processing/overview/2020/12/16/week1-day3.html",
            "relUrl": "/image_processing/overview/2020/12/16/week1-day3.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Week 1, Day 2 (Python for Data Science)",
            "content": "Notebooks and Assignments . Please download all the lectures and assignment notebooks of week 1 (Day 2) here. We have also posted a guide video on downloading &amp; accessing materials on our youtube channel. . Learning Tips . We Strongly recommand learning with playing. We provide notebooks for students to get started. If you get trouble with learning, please feel free to reach out to us on facebook and email. . Data Types . In python, there are built-in data-types, some of which are described below: . Int (1 ,100, 99) | Float (1.0, 3.14, 2.718) | String (&quot;Myanmar&quot;, &quot;Burma&quot;) | Boolean (True, False) | Complex (1+0j) | null (None) | To describe the various properties in the real world, we must use different data types. . e.g., if we want to describe a person&#39;s name, we could use String. Int for his/her age, Float for his/her height in (cm) and Boolean if he/she is graduated or not. . The main take away from this lecture notebook is to know the differences between data types and their usage. . Four Most Used Data Types . We can store data of different types in variable names. See the example below: . # Int int1 = 5 int2 = 3 # Float float1 = 1. float2 = 3.14 # String string1 = &quot;Hello&quot; string2 = &#39;World&#39; # Boolean bool1 = True bool2 = False . print function is used to print the values of the variable to the output screen. . print(int1) . 5 . We can check the datatype of the variable by using type function: . type(int1) . int . Arithmetic Operations on Number Data Types . We could do arithmetic operations on Number Data Types (Int and Float) as shown below: . int1 = 5 int2 = 3 # We can add, subtract two elements of &#39;Int&#39; dtype. print(int1 + int2) # Addition print(int1 - int2) # Subtraction print(int1 * int2) # Multiplication print(int1 / int2) # Division print(int1 % int2) # Remainder print(int1**int2) # Power print(&quot;&quot;) float1 = 1. float2 = 3.14 # We can also, do operations on `Float` dtype. print(float1 + float2) print(float1 - float2) print(float1 * float2) print(float1 / float2) print(float1 % float2) print(float1**float2) . . 8 2 15 1.6666666666666667 2 125 4.140000000000001 -2.14 3.14 0.3184713375796178 1.0 1.0 . String Operations . We can use &#39;+&#39; operator to concatenate two strings. . # Simple concatenation of two strings concatenate_string = string1 + string2 print(concatenate_string) . HelloWorld . We can also use f-Strings method for better formating of string. . In f-Strings method, we can insert variables in the string format. . # f-Strings method. f_string = f&quot;Hello World, welcome to the wonderland&quot; print(f_string) string1 = &quot;Hello&quot; string2 = &quot;World&quot; f_string_var = f&quot;{string1} {string2}, This is testing the f-Strings&quot; print(f_string_var) . . Hello World, welcome to the wonderland Hello World, This is testing the f-Strings . . Important: One thing to be aware of in Python, we can not add two variables of different data types. In the example described below, we try to add Int and String data types, which is not possible. . var_a = &quot;Hello&quot; var_b = 5 print(var_a + var_b) . . TypeError Traceback (most recent call last) &lt;ipython-input-22-7f4a263b8c2b&gt; in &lt;module&gt; 2 var_b = 5 3 -&gt; 4 print(var_a + var_b) TypeError: can only concatenate str (not &#34;int&#34;) to str . # Or this print(int(var_a) + var_b) . . ValueError Traceback (most recent call last) &lt;ipython-input-23-c93ae652aa67&gt; in &lt;module&gt; 1 # Or this -&gt; 2 print(int(var_a) + var_b) ValueError: invalid literal for int() with base 10: &#39;Hello&#39; . When converting String to Int, if the value in that String is Int, then we can do addition. . # we could, for example var_a = &quot;5&quot; var_b = 5 print(int(var_a) + var_b) . . 10 . Boolean Operation . Boolean includes only two values, True and False. . Note: The capital letter and small letter must be correctly spelled.eg. true will not work. Boolean variables are often used in conditional operations, such as if and while. . bool1 = True bool2 = False # Check if a varible is True if bool1: print(&quot;bool1 variable is True&quot;) # This print statement is not working # because bool2 is not True if bool2: print(&quot;bool2 variable is True&quot;) . . bool1 variable is True . # Check if a variable is False (not True) if not bool1: print(&quot;bool1 variable is False&quot;) if not bool2: print(&quot;bool2 variable is False&quot;) . . bool2 variable is False . # We could use AND, OR operation too. if bool1 and bool2: print(&quot;Both Variables are True&quot;) if bool1 or bool2: print(&quot;At least one variable is True&quot;) . . At least one variable is True . Further Resources for Data Types . If you want to learn more about data types and their operations in more details, please visit to official Python documentation. You can also learn more about it in this blog post about Basic Data Types in Python. . Data Structures . Python Data Structures are used to store and collect data. There are four basic built-in data structures in Python. . # Simple Creation of each four types of structures list_obj = [1, 2, 3, 4, 1, 2, 3, 4] set_obj = {1, 2, 3, 4, 1, 2, 3, 4} tuple_obj = (1, 2, 3, 4, 1, 2, 3, 4) dict_obj = {&#39;a&#39;:1, &#39;b&#39;:2, &#39;c&#39;:3, &#39;d&#39;:4, &#39;e&#39;:1, &#39;f&#39;:2} . print(type(list_obj), list_obj) print(type(set_obj), set_obj) print(type(tuple_obj), tuple_obj) print(type(dict_obj), dict_obj) . . &lt;class &#39;list&#39;&gt; [1, 2, 3, 4, 1, 2, 3, 4] &lt;class &#39;set&#39;&gt; {1, 2, 3, 4} &lt;class &#39;tuple&#39;&gt; (1, 2, 3, 4, 1, 2, 3, 4) &lt;class &#39;dict&#39;&gt; {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4, &#39;e&#39;: 1, &#39;f&#39;: 2} . List . The Most commonly used data structure in Python, List, has the following properties : . Element are accessable with order | Mutable (variable values can be changed) | list_height = [170, 172, 174, 160, 178] print(&quot;Heights of student in class...&quot;) print(list_height) . Heights of student in class... [170, 172, 174, 160, 178] . Access with Index . . Note: If we access element that is out of range, it will rasie IndexError as follows . print(&quot;Student A height : &quot;, list_height[0]) print(&quot;Student B height : &quot;, list_height[1]) print(&quot;Student A height : &quot;, list_height[2]) print(&quot;Student B height : &quot;, list_height[3]) print(&quot;Student A height : &quot;, list_height[4]) print(&quot;Student B height : &quot;, list_height[5]) . . Student A height : 170 Student B height : 172 Student A height : 174 Student B height : 160 Student A height : 178 . IndexError Traceback (most recent call last) &lt;ipython-input-4-d5a3a372c0b9&gt; in &lt;module&gt; 4 print(&#34;Student B height : &#34;, list_height[3]) 5 print(&#34;Student A height : &#34;, list_height[4]) -&gt; 6 print(&#34;Student B height : &#34;, list_height[5]) IndexError: list index out of range . IndexError: list index out of range. Because there are only 5 elements in list, list_height[5] request for 6th element, which is out of range. . Access with Iteration(For Loop) . num_student = 0 for height in list_height: num_student += 1 print(height, end = &#39;,&#39;) print(f&quot; nThere are {num_student} students in the class&quot;) . . 170,172,174,160,178, There are 5 students in the class . &#39;&#39;&#39; Simple Program: Convert height information in &quot;cm&quot; to &quot;feet&quot; &#39;&#39;&#39; # Convert cm to feet for height_cm in list_height: height_feet = height_cm * 0.0328 # cm to feet equation print(f&quot;Studnet height : {height_cm} cm {height_feet:.2f} feet&quot;) . . Studnet height : 170 cm 5.58 feet Studnet height : 172 cm 5.64 feet Studnet height : 174 cm 5.71 feet Studnet height : 160 cm 5.25 feet Studnet height : 178 cm 5.84 feet . List with different Datatype . We can store, not just one datatype, but variables with different data types in a list. . student_A = [&quot;Aung Aung&quot;, &quot;McE&quot;, 3, 6] student_B = [&quot;Soe Pyae&quot;, &quot;Civil&quot;, 3, 6] print(&quot;Student A Info : &quot;, student_A) print(&quot;Student B Info : &quot;, student_B) . . Student A Info : [&#39;Aung Aung&#39;, &#39;McE&#39;, 3, 6] Student B Info : [&#39;Soe Pyae&#39;, &#39;Civil&#39;, 3, 6] . # But Be Aware, We do not know which element contain what information.. print(student_A[2] + student_A[3]) print(student_A[0] + student_A[1]) print(student_A[1] + student_A[2]) . . 9 Aung AungMcE . TypeError Traceback (most recent call last) &lt;ipython-input-5-b71be46f223e&gt; in &lt;module&gt; 2 print(student_A[2] + student_A[3]) 3 print(student_A[0] + student_A[1]) -&gt; 4 print(student_A[1] + student_A[2]) TypeError: can only concatenate str (not &#34;int&#34;) to str . TypeError :can only concatenate str to str | int to int This often happen in List when we try to store different data type into one list. It is hard to get exact index for each element. For that, in python, we use Dictionary to store key-value paired information. . Dictionary . When we walk into a library to find books, we search the book by its title, author or published year. Python use the same idea to store value for the varible, by creating the key for that value. Thus Dictionary in python is key-value paired data structure. . # List student_A_list = [&quot;Aung_Paing&quot;, &quot;McE&quot;, 3, 6] student_B_list = [&quot;Soe_Pyae&quot;, &quot;Civil&quot;, 3, 6] # Dictionary student_A_dict = {&quot;Name&quot; : &quot;Aung_Paing&quot;, &quot;Major&quot; : &quot;McE&quot;, &quot;Batch&quot; : 3, &quot;Year&quot; : 6} student_B_dict = {&quot;Name&quot; : &quot;Soe_Pyae&quot;, &quot;Major&quot; : &quot;Civil&quot;, &quot;Batch&quot; : 3, &quot;Year&quot; : 6} print(student_A_list) print(student_A_dict) . . [&#39;Aung_Paing&#39;, &#39;McE&#39;, 3, 6] {&#39;Name&#39;: &#39;Aung_Paing&#39;, &#39;Major&#39;: &#39;McE&#39;, &#39;Batch&#39;: 3, &#39;Year&#39;: 6} . # If we want to get &quot;Name&quot; info for student_A name_student_A = student_A[0] print(name_student_A) print() # But in dict, we just need to specified it. We do not need to remember the index of that info. print(&quot;Name : &quot;, student_A_dict[&quot;Name&quot;]) print(&quot;Major : &quot;, student_A_dict[&quot;Major&quot;]) print(&quot;Batch : &quot;, student_A_dict[&quot;Batch&quot;]) print(&quot;Year : &quot;, student_A_dict[&quot;Year&quot;]) . . Aung_Paing Name : Aung_Paing Major : McE Batch : 3 Year : 6 . Tuple . Sometimes, we need immutable data. For example, Coordinate of a location, your Birthday, your significant other&#39;s Phone Number or may be even your gene code. The data that we want to access but must be changed are stored with Tuple. . home_coordinate = (123, 456) birthday = (11, 1, 1998) print(type(birthday)) print(birthday) . . &lt;class &#39;tuple&#39;&gt; (11, 1, 1998) . # Let&#39;s try to access the month with index... month = birthday[1] print(month) # Let&#39;s try to change the day.... birthday[1] = 10 . . 1 . TypeError Traceback (most recent call last) &lt;ipython-input-11-c728f5c5b1e0&gt; in &lt;module&gt; 4 5 # Let&#39;s try to change the day.... -&gt; 6 birthday[1] = 10 TypeError: &#39;tuple&#39; object does not support item assignment . Thus, we can see the data in tuples cannot be changed . Set . Set as the name suggest, is a set of collection. Sets ignore the order and the number of elements in a collection and only store the representatives in that collection. . set_obj = {1, 2, 3, 1, 2, 3, 4} print(type(set_obj)) print(set_obj) . . &lt;class &#39;set&#39;&gt; {1, 2, 3, 4} . # We cannot access the elements in the set set_obj[0] . . TypeError Traceback (most recent call last) &lt;ipython-input-25-c6b65031723d&gt; in &lt;module&gt; 1 # We could not access the element in that set. -&gt; 2 set_obj[0] TypeError: &#39;set&#39; object is not subscriptable . Tips . Error When you encounter an Error in the code, try the following steps: . Stay calm &amp; Read the error message carefully | Copy and paste the error message in google search or stackoverflow. | Ask for help in forums if necessary. | There are more than 8 million python users in the world and it is very likely that someone has encounterd the same problem as you did. So, don&#39;t hesitate to search for help. . We can check the specification of the variable by: . set_obj? . Type: set String form: {1, 2, 3, 4} Length: 4 Docstring: set() -&gt; new empty set object set(iterable) -&gt; new set object Build an unordered collection of unique elements. . Further Resources for Data Structure . If you would like to know more about Data Structure in Python. Please visit official Python cocumentation. You can also learn more about it in this blog post about Common Python Data Structure . Conditionals &amp; Loops in Python . Conditionals . if Statements in Python allow us to tell the computer to perform alternative actions based on a certain set of results. . In other words, we are telling the computer : &quot;Hey if this case happens, perform some action&quot; . We can then expand the idea further with elif and else statements, which allows us to tell the computer: &quot;Hey if this case happens, perform some action. Else, if another case happens, perform some other action. Else, if none of the above cases happened, perform this action&quot; . Let&#39;s go ahead and look at the syntax format for if-else cases to get a better idea of this: . # Simple if-else case rain = False if not rain: print(&quot;The weather is fine today, I am going out.&quot;) else: print(&quot;It is raining, I cannot go out...&quot;) . . The weather is fine today, I am going out. . # Or this way: if rain==True: print(&quot;It is raining, I cannot go out...&quot;) else: print(&quot;Today weather is fine, I am going out.&quot;) . . Today weather is fine, I am going out. . # sometime, in the program, we have to deal with other conditions rain = False weekday = True if not rain and weekday: print(&quot;I am going to School Today&quot;) elif not rain and not weekday: print(&quot;I am going out to play&quot;) else: # Rain and weekday print(&quot;Even though it is raining, I still have to go to school ...&quot;) . . I am going to School Today . Also, we could check the String and Int variables: . Note: here, when checking if Condition == True, we use == instead of = . # Check String operation = &quot;add&quot; if operation == &quot;add&quot;: print(&quot;Add Operation&quot;) elif operation == &quot;sub&quot;: print(&quot;Sub Operation&quot;) elif operation == &quot;mul&quot;: print(&quot;Multiplication Operation&quot;) elif operation == &quot;div&quot;: print(&quot;Division Operation&quot;) . . Add Operation . # Check Int int_obj = 10 # int_obj = -1 # Comment out to test # int_obj = 0 # int_obj = None if int_obj: print(&quot;There exists value for that int_obj&quot;) else: print(&quot;sorry, the input is None or Zero&quot;) . . There exists value for that int_obj . Loops . There are two main types of loops in python. . for | while | Generally, For iterates for the given range, While checks the condition and continue iteration until condition is False. . for range(): . while condition: . For . A for loop acts as an iterator in Python; it goes through items that are in a sequence or any other iterable item. Objects, that we&#39;ve learned and we can iterate over, include strings, lists, tuples, and even built-in iterables for dictionaries, such as keys or values. . Here&#39;s the general format for a for loop in Python: . for item in object: statements to do things . # Loop for 10 times for i in range(10): print(i, end=&quot; &quot;) print() . . 0 1 2 3 4 5 6 7 8 9 . # Let&#39;s access element from list list_obj = [1, 2, 3, 4, 5, 6, 7, 8, 9] for obj in list_obj: print(obj, end=&quot; &quot;) . . 1 2 3 4 5 6 7 8 9 . Tuples have a special quality when it comes to for loops. If you are iterating through a sequence that contains tuples, the item can actually be the tuple itself, this is an example of tuple unpacking. During the for loop we will be unpacking the tuple inside of a sequence and we can access the individual items inside that tuple. . list1 = [(0,1),(2,3),(4,5)] for tup in list1: print(tup) . . (0, 1) (2, 3) (4, 5) . # Now with unpacking! for (t1,t2) in list1: print(t1, end=&quot; &quot;) . . 0 2 4 . In the above case, the first element of each tuple is printed out. . # Now with unpacking! for (t1,t2) in list1: print(t2, end=&quot; &quot;) . . 1 3 5 . Here, the second element of each tuple gets printed out. . With tuples in a sequence we can access the items inside of them through unpacking! The reason this is important is because many objects will deliver their iterables through tuples. . Nested For Loop . Nested For, means for loop inside a for loop . nested_list = [[1, 2, 3, 4], [5, 6, 7, 8]] # To access nested list, we could use nested for loop for row in nested_list: for col in row: print(col, end=&quot; &quot;) . . 1 2 3 4 5 6 7 8 . While . The while statement in Python is one of most general ways to perform iteration. A while statement will repeatedly execute a single statement or group of statements as long as the condition is true. The reason it is called a &#39;loop&#39; is because the code statements are looped through over and over again until the condition is no longer met. . The general format of a while loop is: . while condition: code statements else: final code statements . Let’s look at a few simple while loops in action. . # While loop and loop for 10 times. i = 0 while i&lt;10: print(i, end=&quot; &quot;) i+=1 . . 0 1 2 3 4 5 6 7 8 9 . x = 0 while x &lt; 5: print(&#39;x is currently:&#39;,x) print(&#39;x is still less than 5, keep adding 1 to x&#39;) x += 1 . . x is currently: 0 x is still less than 5, keep adding 1 to x x is currently: 1 x is still less than 5, keep adding 1 to x x is currently: 2 x is still less than 5, keep adding 1 to x x is currently: 3 x is still less than 5, keep adding 1 to x x is currently: 4 x is still less than 5, keep adding 1 to x . Notice how many times the print statements occurred and how the while loop kept going until the True condition was met, which occurred once x==5. It&#39;s important to note that once this occurred the code stopped. Let&#39;s see how we could add an else statement: . x = 0 while x &lt; 5: print(&#39;x is currently: &#39;,x) print(&#39;x is still less than 5, keep adding 1 to x&#39;) x += 1 else: print(&#39;All Done!&#39;) . . x is currently: 0 x is still less than 5, keep adding 1 to x x is currently: 1 x is still less than 5, keep adding 1 to x x is currently: 2 x is still less than 5, keep adding 1 to x x is currently: 3 x is still less than 5, keep adding 1 to x x is currently: 4 x is still less than 5, keep adding 1 to x All Done! . When the condition isn&#39;t true anymore, that is, x is now 5, and so the else statement All Done! is printed out instead. . Further Resources for Conditionals and Loops . If you want to learn more about conditional statements and loops in more details, please visit to official Python documentation. You can also learn more about conditional statements, for loops and while loops in these posts. . Functions . Functions are great arsenal in python. . You could think of functions as a factory, process (do function) the raw inputs (Arguments) to desired products (output). There are two types of functions in Python. . Built-in Functions | User-Defined Functions | Built-in Functions . We&#39;ve already seen a few example of built-in functions when learning about Data Types in Python. Built-in Functions, come along with python and could be very useful when you need them. . Following are some examples of built-in functions. . int_obj = -5 print(int_obj) # Find the absolute value abs_int = abs(int_obj) print(abs_int) # Convert Int to String int2str = str(int_obj) print(int2str, type(int2str)) # Check the datatype print(isinstance(int_obj, int)) . . -5 5 -5 &lt;class &#39;str&#39;&gt; True . We can also use built-in functions to construct data-structures. . set_obj = set((1, 2, 3, 1, 2, 3)) print(set_obj) list_obj = list((1, 2, 3, 1, 2, 3)) print(list_obj) tuple_obj = tuple((1, 2, 3, 1, 2, 3)) print(tuple_obj) dict_obj = dict([(1,&quot;a&quot;), (2, &quot;b&quot;)]) print(dict_obj) . . {1, 2, 3} [1, 2, 3, 1, 2, 3] (1, 2, 3, 1, 2, 3) {1: &#39;a&#39;, 2: &#39;b&#39;} . . User-Defined Functions . This section will consist of explaining what a function is in Python and how to create one. Functions will be one of our main building blocks when we construct larger and larger amounts of code to solve problems. . . Typical function in python consists of the following parts: . Input (Arguments in functions) | Process (What this function do) | Output (What this function return) | In short, IPO for constructing a function. . Functions will be one of most basic levels of reusing code in Python, and it will also allow us to start thinking of program design (we will dive much deeper into the ideas of design when we learn about Object Oriented Programming). . def Statements . Let&#39;s see how to build out a function&#39;s syntax in Python. It has the following form: . def function_name(arg1,arg2): &#39;&#39;&#39; This is where the function&#39;s Document String (docstring) goes &#39;&#39;&#39; # Do stuff here # Return desired result . We begin with def then a space, followed by the name of the function. Try to keep names relevant though they can be variable, for example len() is a good name for a length() function. Also be careful with names, you wouldn&#39;t want to call a function the same name as a built-in functions(such as len). . Next comes a pair of parentheses with a number of arguments separated by a comma. These arguments are the inputs for your function. You&#39;ll be able to use these inputs in your function and reference them. After this you put a colon. . Now here is the important step, you must indent to begin the code inside your function correctly. Python makes use of whitespace to organize code. Lots of other programing languages do not do this, so keep that in mind. . Let&#39;s see the example of creating basic printing function. . # Define a function # No argument def print_hello(): print(&quot;Hello World&quot;) print_hello() # Call the funtion . . Hello World . Let&#39;s see some functions with input arguments. . # Argument with user name def print_hello(name): &quot;&quot;&quot; Print The User Name Args: name(String) -&gt; User name Return: None &quot;&quot;&quot; print(f&quot;Hello {name}! Welcome.&quot;) print_hello(&quot;Aung Aung&quot;) # Call the funtion . . Hello Aung Aung! Welcome. . # Arguments with different datatypes def print_hello(name, age): &quot;&quot;&quot; Print The User Name Args: name (String) -&gt; User name age (Int) -&gt; User Age Return: None &quot;&quot;&quot; print(f&quot;Hello {name}! Welcome. You are now {age} years old.&quot;) print_hello(&quot;Aung Aung&quot;, 22) # Call the funtion . . Hello Aung Aung! Welcome. You are now 22 years old. . Functions arguments can also have default values. See the example below. . # Argument having default value def print_hello(name, age = 18): &quot;&quot;&quot; Print The User Name Args: name (String) -&gt; User name age (Int) -&gt; User Age Return: None &quot;&quot;&quot; print(f&quot;Hello {name}! Welcome. You are now {age} years old.&quot;) # Because the &#39;age&#39; argument has default value, we don&#39;t need to provide value. print_hello(&quot;Aung Aung&quot;) print_hello(&quot;Aung Aung&quot;, 22) . . Hello Aung Aung! Welcome. You are now 18 years old. Hello Aung Aung! Welcome. You are now 22 years old. . Using return . Let&#39;s see some examples that use a return statement. return allows a function to return a result that can then be stored as a variable, or used in whatever manner a user wants. . def add_num(num1,num2,num3): return num1 + num2 + num3 . # Calling the function add_num(1,2,3) . . 6 . # Can also save as variable due to return result = add_num(1,2,3) print(result) . . 6 . # Let&#39;s try to make a power function def power(base_num, pow_num): &quot;&quot;&quot; Calculate the power of base_num Args: base_num(Int) -&gt; Base number for calculation pow_num(Int) -&gt; Power number for calculation Return: power_num(Int) -&gt; Power number for calculation &quot;&quot;&quot; return base_num ** pow_num # We can directly print out the value print(power(2, 3)) # Or we can assign the output value to a variable and print. output = power(3, 2) print(output) . . 8 9 . In the above function: We get the process of Calculate the Power of base_num, Input of base_num and pow_num. Output for power_num. . Further Resources for Functions . If you want to learn more about built-in functions in more details, please visit to official Python documentation. You can also check this blog post about Functions. . Classes . Classes are the main building blocks in Object Oriented Programming (OOP). OOP is one of the hardest parts for beginners when they are first starting to learn Python. But we will get you through OOP in the following section. So, let&#39;s start. . . Intro to OOP . Let&#39;s start the lesson by remembering about the Basic Python Objects. For example: . lst = [0, 1, 2, 3] . When we print out the type of that lst . print(type(lst)) . &lt;class &#39;list&#39;&gt; . This is the built-in Class of list. And we can call methods on that list with . lst.append(2) lst . [0, 1, 2, 3, 2] . Here, append is the method that the &lt;class &#39;list&#39;&gt; has. . Objects . Everything in Python is an object. We can use type() to check the type of object: . print(type(1)) print(type([])) print(type(())) print(type({})) . . &lt;class &#39;int&#39;&gt; &lt;class &#39;list&#39;&gt; &lt;class &#39;tuple&#39;&gt; &lt;class &#39;dict&#39;&gt; . So we know all these things are objects, so how can we create our own Object types? That is where the class keyword comes in. . Class and Attributes . class . User defined objects are created using the class keyword. The class is a blueprint that defines the nature of a future object. From classes we can construct instances. An instance is a specific object created from a particular class. For example: . t = (1, 2, 3) . We created the object t which was an instance of a tuple object. . Now, Let&#39;s try a step up and create our own class. . # Construct a new object type called Student class Student: pass # Instance of the class Student x = Student() print(type(x)) . &lt;class &#39;__main__.Student&#39;&gt; . Most of the programmers give classes a name that starts with a capital letter by convention. Note how x is now the instance of a Student class. In other words, we instantiate the Student class. . At the inside of the class we currently just have pass keyword. But we can define class attributes and methods. . An attribute is a characteristic of an object.A method is an operation we can perform with the object. . For example, we can create a class called Dog. An attribute of a dog may be its breed or its name, while a method of a dog may be defined by a .bark() method which returns a sound. . Attributes . The syntax for creating an attribute is: . self.attribute = something . There is a special method called: . __init__() . This method is used to initialize the attributes of an object. For example: . class Student: def __init__(self, name): self.name = name a = Student(name = &#39;Aung Paing&#39;) b = Student(name = &#39;Soe Pyae Phyo&#39;) . Let&#39;s break down what we have above. The special method . __init__() . is called automatically right after the object has been created. . def __init__(self, name): . Each attribute in a class definition begins with a reference to the instance object. It is by convention named self. The name is the argument. The value is passed during the class instantiation. . self.name = name . . Note: self.name can be given any desire variable names; no need to be name. For example, self.student_name = name Now we have created two instances of the Student class. With two different names, we can then use these attributes like this: . a.name . &#39;Aung Aung&#39; . b.name . &#39;Soe Soe&#39; . Note how we don&#39;t have any parentheses after name; this is because it is an attribute and doesn&#39;t take any arguments. . Lets add more attributes to our Student class. . class Student: def __init__(self, name, major, batch, age, passedlastterm): self.name = name self.major = major self.batch = batch self.age = age self.passedlastterm = passedlastterm student_a = Student(&quot;Myo Myo&quot;, &quot;EC&quot;, 3, 24, True) student_b = Student(&quot;Su Su&quot;, &quot;Text&quot;, 2, 18, False) print(&quot; nStudent a Data :&quot;) print(student_a.name) print(student_a.major) print(student_a.batch) print(student_a.age) print(student_a.passedlastterm) . . Student a Data : Myo Myo EC 3 24 True . Methods . Methods are functions defined inside the body of a class. They are used to perform operations with the attributes of our objects. Methods are a key concept of the OOP pattern. They are essential to dividing responsibilities in programming, especially in large applications. . Let&#39;s go through an example of creating a Square class: . class Square: # Square gets instantiated def __init__(self, length = 4): self.length = length self.area = length * length # Resetting length def setLength(self, new_length): self.length = new_length self.area = new_length * new_length s = Square() print(&#39;Length is &#39;, s.length) print(&#39;Area is &#39;, s.area) s.setLength(6) print(&#39;Now length is &#39;, s.length) print(&#39;Now area is &#39;, s.area) . . Length is 4 Area is 16 Now length is 6 Now area is 36 . In this Square class, we have defined two attributes: self.length and self.area. . def setLength(self, new_length): . is called method of the class, which we use to interact with the user and manipulate the class attributes. Notice how we used self. notation to reference attributes of the class within the method calls. . Now, Let&#39;s add some methods to Student class. . class Student: def __init__(self, name, major, batch, age, passedlastterm): self.name = name self.major = major self.batch = batch self.age = age self.passedlastterm = passedlastterm def print_result(self): if self.passedlastterm: print(&quot;Congrats, you can move to next term.&quot;) else: print(&quot;Sorry, you need to retake the exam.&quot;) student_a = Student(&quot;Myo Myo&quot;, &quot;EC&quot;, 3, 24, True) print(&quot;Student a Result&quot;) student_a.print_result() . . Student a Result Congrats, you can move to next term. . Inheritance . Inheritance is a way to form new classes using classes that have already been defined. The newly formed classes are called derived classes, the classes that we derive from are called base classes. Important benefits of inheritance are code reuse and reduction of complexity of a program. The derived classes override or extend the functionality of base classes. . Let&#39;s use our Student class again. . # Inheritance and not change anything. class ExchangeStudent(Student): pass . In this example, we have two classes: Student and ExchangeStudent. The Student is the base class, the ExchangeStudent is the derived class. . The derived class inherits the functionality of the base class. This is shown in the example below. . # ExchangeStudent should have all properties and methods Student has. student_ex = ExchangeStudent(&quot;Htet Htet&quot;, &quot;Civil&quot;, 1, 25, True) print(&quot; nStudent ex Result&quot;) student_ex.print_result() . Student ex Result Congrats, you can move to next term. . Further Resources for Class . If you want to learn more about Object Oriented Programming (OOP) in more details, please visit to official Python documentation. You can also check this blog post about OOP. . Icons made by Freepik from www.flaticon.com",
            "url": "https://ytu-cvlab.github.io/mce-51069/python/2020/12/14/week1-day2.html",
            "relUrl": "/python/2020/12/14/week1-day2.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Week 1, Day 1 (Introduction to the course)",
            "content": "Anaconda and Jupyter Notebook . Anaconda is a Data Science Platform, which comes with Data Science related packages and libraries, such as numpy and matplotlib, in Python. The aforementioned reason for using Anaconda is just one of the many good reasons and there are certainly other benefits. If you are interested in it, Please visit their website to explore more. . In this course, we decide to get Anaconda installed for every student, and make the installation video guides for Anaconda available in this website. . Jupyter Notebook is an open source, web application that allow users to create and share documents that contain live codes, equations, visualizations and narrative text. Quote :Jupyter.org . Anaconda Installation on Windows . . Anaconda Installation on Mac . . How to use Jupyter Notebook . . Student ID and Attendance . Please fill out this form from this link to get your student ID. . We will record attendance in every session with google forms provided only in zoom chat. . Assignment . Please submit your week 1 assignments from this link. We will return your assignment scores after the deadline day (you can check on your submission form). Please note that we will return your final grade only if you submit all the assignments. .",
            "url": "https://ytu-cvlab.github.io/mce-51069/assignment/installation_guide/student_id/2020/12/12/week1-day1.html",
            "relUrl": "/assignment/installation_guide/student_id/2020/12/12/week1-day1.html",
            "date": " • Dec 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": ".",
          "url": "https://ytu-cvlab.github.io/mce-51069/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ytu-cvlab.github.io/mce-51069/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}