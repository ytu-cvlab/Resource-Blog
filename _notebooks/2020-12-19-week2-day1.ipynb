{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, Day 1 (Introduction to Artificial Intelligence and Computer Vision)\n",
    "> Welcome to first day (Week 2) of the McE-51069 course. In this week, we will walk you through with basic Knowledge on Deep learning and Computer Vision.\n",
    "- sticky_rank: 1\n",
    "- toc: true\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [deep_learning, computer_vision]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence\n",
    "\n",
    "Human has always been fascinated by the ideas of putting the intelligence to the machine. Still, it was until 1956, the term \"Artificial Intelligence\" was coined, at a conference at Dartmouth College, in Hanover, New Hampshire. At such time, people were very optimistic about the future of AI. And the fundings and interest invest in the field get larger and larger.<br>\n",
    "But making a machine to behave just like a human is not an easy task. With the researchers fail to deliver the promise, and also with several reports criticizing progress in AI, the funding and interest in the field get dropped off, people later often refers this as the \"AI winter\" which happend during 1974s-1980.\n",
    "\n",
    "Even though there are some private and public fundings to the field, the whole hype about \"Aritificial Intelligence\" gets cooled down. And around 1987s to 1993, the field experienced another \"winter\".\n",
    "\n",
    "It was only after 1997, IBM's Deep Blue became the first computer to beat a chess champion, Russian grandmaster Garry Kasparov, that the term \"AI\" is coming back to the reserach ground.\n",
    "\n",
    "The field of artificial intelligence finally had its breakthrough moment in 2012 at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) with the introduction of Alex-Net. (ILSVRV) is a vision competition and before Alex-Net, the error rate hover around 26%. But with Alex-Net, the error rate comes down to only 16.4%. That is a huge accomplishment and people now see hope in Deep Neural Networks Again.\n",
    "\n",
    "\n",
    "Nowaday, when people talk about \"Artificial Intelligence\", they often refers to \"Deep Neural Networks\", a branch of Machine learning that imitates the human brain cells as to function and process data. And with modern hardware advanced, big data accessable and algorithms optimized, the field of Deep Learning is getting more and more research interestes and improvements everyday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "\n",
    "The main purpose of computer vision is for computer to have the \"vision\", ability to perceive the world. To do that, we have to know how human vision system work. It is now known that, human vision is hierarchical. Neurons detect simple features like edges, then feed into more complex features like shapes, and then eventually feed into more complex visual representations.\n",
    "\n",
    "![](images/percept.png)\n",
    "Today, we have the AI algorithms, that mimic the hierarchical architecture of the human vision system called the CNN(Convolutional Neural Networks). CNNs use convolutional layers to extract the features in images and then use fully connected layers for the output.\n",
    "\n",
    "![](images/cnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Available Today\n",
    "\n",
    "Even though many people think that AI is far away from our daily life, it is not quite true. When we scroll on social Media, let's say Facebook, we get the newsfeed from Facebook's recommandation AI. And when we travel using \"Google Maps\", it automatically generates the traffic conditions using AI. What we didn't realize is, AI has already been a part of our daily lives and improving our life quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "Neural Network is the basic building block for the Deep Neural Networks.\n",
    "\n",
    "Let's first do a simple classification on two data points.<br>\n",
    "Given the x-coordinate of the data point, we will have to classify wherether this point belongs to red or blue. For that, we will need to find the threshold value, decision boundary or decision surface, whatever you may call it. For this particular example, the threshold value would be 2.5.\n",
    "\n",
    "So, we would write this with the logic of taking x-coordinate as the input,x, and check if it is smaller than 2.5, if `True`, then, the color is \"Red\", and if `False`, the color is \"Blue\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/log_1.jpeg)\n",
    "\n",
    "We can also write the function instead of logic with **x-2.5**, and check if positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/log_2.jpeg)\n",
    "But, still, we do not know the confidence of prediction value by the above equation. For such purposes, Sigmoid function is introduced. It keeps the output range from 0~1. So, if the output from sigmoid is near 1, let's say, 0.9, we would say that this point is 90% sure to be the \"Blue\" Point. And if the output from the sigmoid is 0.2, we would say this point is 20% sure to be the \"Blue\" Point or (1-0.2 = 0.8) 80% sure to be the \"Red\" Point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/log_3.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/log_4.jpeg)\n",
    "\n",
    "Let's apply this logic with 2 dimensions(x and y).\n",
    "Remember when we introduced the threshold value, we refered the point as the decision boundary or decision surface. That is because that value(threshold) makes the decision for the model in a single dimension.<br>\n",
    "For 2 dimensional cases, we will need a *Line (2D)* instead of *Point (1D)* as a decision boundary for classification.\n",
    "\n",
    "So, as the dimension for the dataset increases, the dimension of the decision surface also increases.<br>\n",
    "In 1D : \\\\((x-2.5)\\\\)  :  Only 1 variable<br> \n",
    "In 2D : \\\\((0.5x -y -1)\\\\) :  2 variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/log_5.jpeg)\n",
    " For this particular example, let's say, the equation for the line is \\\\( y = 0.5x - 1\\\\), then, it can be derived to \\\\( 0.5x - y - 1\\\\). Here, we introduce \"Weight\", the coefficient of variables, and \"Bias\", which is a constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/log_6.jpeg)\n",
    "And with the extra loss function, the basic logistic regression model is developed. Loss function for the model determines how badly the model is performing. The loss function is also known as \"Binary cross-entropy\" Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/log_7.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the Loss of the model, We would like to Update(Optimize) the model for better classification performance. To update the model, gradient descent method is introduced.\n",
    "\n",
    "First, the variables(parameters) that we can update in the model are \"weight\" and \"bias\". So, we need to know, how the change in weight effect loss. Mathematically, this is called the derivative. So, we would like to know \\\\( \\frac{dL}{dW}\\\\).\n",
    "\n",
    "And since the equation for the loss is:<br>\n",
    "\\\\(L = -[ylog\\hat{y} + (1-y)log(1-\\hat{y})]\\\\), L depends on \\\\(\\hat{y}\\\\)<br>\n",
    "\\\\(\\hat{y} = \\sigma{z}\\\\), \\\\(\\hat{y}\\\\)  depends on z<br>\n",
    "\\\\(Z = W^TX + B\\\\), Z depands on W,<br>\n",
    "\n",
    "We can use chain Rule to find the derivative of L and W with the following equation.\n",
    "\n",
    "\\\\(\\frac{dL}{dW} = (y - \\hat{y}) * X\\\\)\n",
    "\n",
    "If you want to know the exact steps of derivation, here is this [link](https://math.stackexchange.com/questions/2503428/derivative-of-binary-cross-entropy-why-are-my-signs-not-right) from stack exchange.\n",
    "\n",
    "After that, we can update the weight values with gradient descent equation:<br>\n",
    "\\\\(W = W - \\alpha \\frac{dL}{dW}\\\\)<br>\n",
    "\\\\(\\alpha\\\\) here means the learning rate, which is a hyperparameter we can adjust to get the better convergence rate for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/log_8.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward Propagation\n",
    "\n",
    "Forward propagation is when the data points pass through the model and output the loss value. Backward propagation happen to update the model parameters(W) with the loss or cost, accumulation of loss value gathered during the forward propagation.\n",
    "\n",
    "![](images/forward.jpeg)\n",
    "![](images/backward.jpeg)\n",
    "\n",
    "The reason why this is called the gradient descent is becasue \\\\( \\frac{dL}{dW}\\\\) will approach to zero, minimal value, while updating the model parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](animations/gradient_descent.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "What happens if we add more units of cells to the previous model?\n",
    "\n",
    "The main problem with just simply stacking cells is that the model is still acting linearly. So, we need to introduce some kind of non-linearity to the model, by adding \"activation function\" to each output of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/dnn_1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will introduce Relu (Rectified Linear Unit), which behaves just like linear function if the output is higher than 0.\n",
    "\n",
    "The equation for the relu function is as follow:\n",
    "\n",
    "\\\\(relu(z) = \n",
    "\\begin{cases}\n",
    "z & if\\ z>0\\\\\n",
    "0 & if\\ z<=0\n",
    "\\end{cases}\\\\)<br>\n",
    "\n",
    "\n",
    "And the derivative for the relu is <br>\n",
    "\\\\(\\frac{d}{dz}relu(z) = \n",
    "\\begin{cases}\n",
    "1 & if\\ z>0\\\\\n",
    "0 & if\\ z<=0\n",
    "\\end{cases}\\\\)\n",
    "\n",
    "The main advantage of using relu is that it does not activate all the neurons at the same time. When the value of z is negative, relu turns it off, returning the zero value, indicating that this feature is not important for the neurons to learn. And since only a certain number of neurons get activated, it is far more computationally efficient than sigmoid and other activation functions.\n",
    "![](images/log_relu.jpeg)\n",
    "\n",
    "If you would like to learn more about various Activation functions, please visit this [blog post](https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap\n",
    "\n",
    "To conclude of what we have learned, we need several things to construct a neural network: \"Weights and Bias\", \"Activation Functions\" and \"Loss Function\".\n",
    "\n",
    "To update(learning) the model, the optimization method \"Gradient Descent\" was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "\n",
    "If you would like to learn more about neural networks, you can visit [Deeplearning.ai](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w) on youtube."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
