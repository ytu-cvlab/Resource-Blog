{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3, Day 2 (YOLO Model Training and Evaluation)\n",
    "> Welcome to second day (Week 3) of the McE-51069 course.\n",
    "- sticky_rank: 8\n",
    "- toc: true\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [deep_learning, computer_vision]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook's from today's lecture is the based on the original yolov5 notebook modified by [Roboflow](https://app.roboflow.com/). You can access to the notebook from this [link](http://colab.research.google.com/github/ytu-cvlab/mce-51069-week3-day1/blob/main/Tutorial_Custom_Roboflow_YOLOv5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO (You Only Look Once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before YOLO paper was released, region proposal networks combined with CNNs are usually used for object detection. These RPNs have high precision but takes a lot of time to train. Many [attempts](https://papers.nips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf) has been made to increase the speed of R-CNNs in the mid to late 2010s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOLO](https://arxiv.org/abs/1506.02640) was introduced in 2015 and showed promise with its speed although it couldn't outperform other detection methods. Later, YOLOv3 was introduced and it made a breakthrough in object detection, outperforming all the object detectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Convolutional Neural Network process the image the same way we human process the visual signal from our real world. It is a hierarchy process, in which we first detect shape, the structure and then object.\n",
    "\n",
    "The model architecture of the Convolutional Neural Network is constructed just as the same as to first detect shape in the image, then the structure, and finally the object. By the difference process step in the model, it can be briefly divided into three part.\n",
    " 1. Backbone (To detect the basic visual data, such as shape, line)\n",
    " 2. Neck (Increase receptive field and connect higher layer feature with lower layer feature.)\n",
    " 3. Head (Perform specific task : object detection, semantic segmentation, etc.)\n",
    " \n",
    " \n",
    "### Backbone\n",
    "\n",
    "In the research field of Deep learning in convolutional neural network, there are many state of the art backbone models that is proved to be useful in many difference field. Because of the basic visual information for all image data are similar, that is, shape, line and edge, so when researcher comes up with some idea of making a model, they often choose one of these state of the art as their backbone.\n",
    "\n",
    "There are many backbone models available in computer vision field. Some of them are:\n",
    " 1. VGG model [paper](https://arxiv.org/pdf/1409.1556.pdf)\n",
    " 2. ResNet model [paper](https://arxiv.org/abs/1512.03385)\n",
    " 3. MobileNet [paper](https://arxiv.org/abs/1704.04861)\n",
    " 4. CSPNet [paper](https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "\n",
    "### Neck\n",
    "\n",
    "To train a complex model, it is convention to add more layers to the model, while it can increase the model complexity, the model tend to forget early information in the later part of model. To avoid this, Neck layer connect between low level layers with high level layers, so that the early information in the model can last till the end of the model. Also, neck layer increase the receptive field of the model by various method. The most popular method is called the [SPP](https://arxiv.org/abs/1406.4729)(Spatial Pyramid Pooling) module, where the model use difference kernel size to convolute the output from the backbone. The neck layer for YOLOv4 is the comibination of [PANet](https://arxiv.org/abs/1803.01534) (Path-Aggregation Net) and [SPP](https://arxiv.org/abs/1406.4729)(Spatial Pyramid Pooling) module.\n",
    "\n",
    "\n",
    "### Head\n",
    "\n",
    "The head of the model determine the task of the model. For example, for image classification model, the head would output the number of class in that dataset. While in the object detection, the head would output the bounding box location, the class of that bounding box and its confidence score, etc. The head of the model combine information feed from the neck and make the decision for the model.\n",
    "The head used in YOLO is called the YOLO head, where it performs only once for object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How YOLO works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yolo uses grid cells to identify objects. All grid cells are processed at once: hence, You Look Only Once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/grid_cells.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N anchor boxes are then introduced to each grid cell to detect the objects in a grid cell. The boxes can also extend outside of each grid cell if the centroid of the detected object falls into its region "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Anchor_boxes.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the YOLO layer is determined by objectness, annotation dimensions and class confidence level for each class. Let's assume that we are detecting three classes. Thus, if we have 3x3 grid cells with 2 anchors, the output will be (3 x 3 x 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/yolo_output.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/yolo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-max supression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, each grid cell on the image generate n anchor boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/generated_anchors.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " YOLO then performs non-max supression on that anchor boxes to remove the anchor boxes with lower confidence levels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nms.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/yolo_recap.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As research efforts are poured into YOLO, developvers are putting more focus into optimizing the model. The fastest versions of YOLO [v4](https://github.com/pjreddie/darknet) and [v5](https://github.com/ultralytics/yolov5) are regarded as state of the art object detection models available today and will remain in the top spot for the near future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://app.roboflow.com/\n",
    "\n",
    "https://www.coursera.org/lecture/convolutional-neural-networks/yolo-algorithm-fF3O0\n",
    "\n",
    "https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#6-visualize\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/08/selecting-the-right-bounding-box-using-non-max-suppression-with-implementation/\n",
    "\n",
    "https://paperswithcode.com/method/yolov4\n",
    "\n",
    "https://medium.com/towards-artificial-intelligence/yolo-v5-explained-and-demystified-4e4719891d69\n",
    "\n",
    "https://www.coursera.org/lecture/convolutional-neural-networks/anchor-boxes-yNwO0\n",
    "\n",
    "https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006\n",
    "\n",
    "https://github.com/ultralytics/yolov5\n",
    "\n",
    "https://engineering.fb.com/2016/08/25/ml-applications/segmenting-and-refining-images-with-sharpmask/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
